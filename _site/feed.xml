<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-02T20:05:20+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Terra Nullius</title><subtitle>Alexei Lednev&apos;s personal blog</subtitle><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><entry><title type="html">Kubernetes 1.33: Resizing Pods Without the Drama (Finally!) üéâ</title><link href="http://localhost:4000/kubernetes/2025/04/26/pod-resize-in-place.html" rel="alternate" type="text/html" title="Kubernetes 1.33: Resizing Pods Without the Drama (Finally!) üéâ" /><published>2025-04-26T19:00:00+03:00</published><updated>2025-04-26T19:00:00+03:00</updated><id>http://localhost:4000/kubernetes/2025/04/26/pod-resize-in-place</id><content type="html" xml:base="http://localhost:4000/kubernetes/2025/04/26/pod-resize-in-place.html">&lt;h2 id=&quot;kubernetes-133-resizing-pods-without-the-drama-finally-&quot;&gt;Kubernetes 1.33: Resizing Pods Without the Drama (Finally!) üéâ&lt;/h2&gt;
&lt;p&gt;Remember that feeling? You meticulously configured your Kubernetes pods, set the CPU and memory just right (or so you thought), only to have your application start gasping for air or hogging resources like it&apos;s Black Friday for RAM. In the old days, the only cure was a full pod restart ‚Äì a disruptive event that often felt like performing open-heart surgery with a butter knife while your SRE team watched through the OR window.&lt;/p&gt;
&lt;p&gt;Well, good news, fellow Kubernetes wranglers! Version 1.33 has landed, and it brings with it a feature that many of us have been dreaming about: &lt;strong&gt;&lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/&quot;&gt;in-place pod vertical scaling&lt;/a&gt;&lt;/strong&gt;! Yes, you read that right. You can now adjust the CPU and memory of your running pods without the dreaded restart. &lt;em&gt;Cue the confetti cannons and slightly-too-enthusiastic DevOps high-fives!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is particularly exciting if you, like me, have been pondering the nuances of &lt;a href=&quot;https://www.linkedin.com/pulse/vpa-kubernetes-autoscaler-could-doesnt-yet-alexei-ledenev-dkrbf&quot;&gt;vertical pod autoscaling&lt;/a&gt;. While VPA is fantastic in theory, its current &amp;quot;recreate&amp;quot; mode can be a bit‚Ä¶ dramatic. In-place resize offers a more graceful way to adjust resources, potentially paving the way for a smoother VPA experience down the line.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;why-this-feature-is-bigger-than-my-morning-coffee-&quot;&gt;Why This Feature is Bigger Than My Morning Coffee ‚òï&lt;/h2&gt;
&lt;p&gt;For us Kubernetes folks, this feature is a game-changer. Imagine a scenario where your application suddenly experiences a surge in traffic. Previously, you&apos;d either have to over-provision (costly!) or trigger a VPA update that would bounce your pods. Now, you can simply nudge the CPU and memory upwards on the fly, keeping your application happy and your users even happier.&lt;/p&gt;
&lt;p&gt;Think about stateful applications, databases, or even those finicky Java apps that need a little extra juice during startup. In-place resize can minimize downtime and provide a much more seamless scaling experience. Let&apos;s break down why:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No More Pod Restart Roulette&lt;/strong&gt;&lt;br /&gt;
Previously, adjusting resources meant playing Russian roulette with your uptime. The &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/autoscaling/&quot;&gt;Vertical Pod Autoscaler (VPA)&lt;/a&gt; would terminate pods like an overzealous bouncer at closing time. Now? We can resize resources smoother than a barista crafting latte art.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost Optimization Magic&lt;/strong&gt;&lt;br /&gt;
Over-provisioning &amp;quot;just in case&amp;quot; becomes less necessary. As the &lt;a href=&quot;https://sysdig.com/blog/kubernetes-1-33-whats-new/&quot;&gt;Sysdig team notes&lt;/a&gt;, this enables true pay-as-you-grow cloud economics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stateful Workload Salvation&lt;/strong&gt;&lt;br /&gt;
Databases no longer need to choose between performance and availability. It&apos;s like changing tires on a moving car ‚Äì risky, but now possible!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;under-the-hood-how-kubernetes-performs-resource-jiu-jitsu-&quot;&gt;Under the Hood: How Kubernetes Performs Resource Jiu-Jitsu ü§º&lt;/h2&gt;
&lt;p&gt;Let‚Äôs geek out on the technical magic‚Äîcomplete with diagrams even your non-tech PM will adore:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart TD
    A[You: kubectl patch pod] --&amp;gt; B[&amp;quot;API Server: ‚ÄòHey Kubelet!‚Äô&amp;quot;]
    B --&amp;gt; C{&amp;quot;Kubelet Checks\nNode Capacity&amp;quot;}
    C --&amp;gt;|Approved| D[CRI Update via containerd/CRI-O]
    C --&amp;gt;|Denied| E[&amp;quot;PodResizePending &amp;lt;br/&amp;gt; ‚ÄòTry again later!‚Äô&amp;quot;]
    D --&amp;gt; F[Live Container Resized!\nNo restart needed]
    style D fill:#bbf,stroke:#333,stroke-width:2px
    style F fill:#bfb,stroke:#333,stroke-width:2px
    style E fill:#fbb,stroke:#333,stroke-width:2px
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;whats-really-happening&quot;&gt;What‚Äôs Really Happening&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mutable Resource Fields&lt;/strong&gt;&lt;br /&gt;
Thanks to &lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/1287&quot;&gt;KEP-1287&lt;/a&gt;, the &lt;code&gt;resources.requests&lt;/code&gt; and &lt;code&gt;resources.limits&lt;/code&gt; in your pod spec are now writable on the fly. No more spec immutability debates!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Kubelet‚Äôs Quick Math Check&lt;/strong&gt;&lt;br /&gt;
When you submit a patch, the kubelet calculates:&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(Node‚Äôs allocatable capacity) 
‚Äì (Sum of all existing container allocations) 
‚â• (Your new request)?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If &lt;strong&gt;yes&lt;/strong&gt;, proceed; if &lt;strong&gt;no&lt;/strong&gt;, emit &lt;code&gt;PodResizePending&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CRI Handshake&lt;/strong&gt;&lt;br /&gt;
The kubelet uses the Container Runtime Interface (CRI) to tell containerd or CRI-O, ‚ÄúHey, give this container more (or less) CPU/memory.‚Äù The runtime adjusts cgroups accordingly‚Äîno restart, no sweat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Status Updates&lt;/strong&gt;&lt;br /&gt;
You‚Äôll get two slick new conditions in &lt;code&gt;kubectl describe pod&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PodResizePending&lt;/strong&gt; ‚Äì ‚ÄúNode‚Äôs busy; try again later.‚Äù&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PodResizeInProgress&lt;/strong&gt; ‚Äì ‚ÄúI got this‚Äîexpanding resources now.‚Äù&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;hands-on-break-things-safely-&quot;&gt;Hands-On: Break Things (Safely!) üîß&lt;/h2&gt;
&lt;h3 id=&quot;inspect-current-resources&quot;&gt;1. Inspect Current Resources&lt;/h3&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pod my-hungry-pod &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-A3&lt;/span&gt; resources
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;See your &lt;code&gt;requests&lt;/code&gt; and &lt;code&gt;limits&lt;/code&gt; under the container spec.&lt;/p&gt;
&lt;h3 id=&quot;basic-in-place-resize&quot;&gt;2. Basic In-Place Resize&lt;/h3&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Give my-pod a caffeine boost&lt;/span&gt;
kubectl patch pod my-hungry-pod &lt;span class=&quot;nt&quot;&gt;--subresource&lt;/span&gt; resize &lt;span class=&quot;nt&quot;&gt;--patch&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;s1&quot;&gt;&apos;{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;your-container-name&quot;,&quot;resources&quot;:{&quot;requests&quot;:{&quot;cpu&quot;:&quot;500m&quot;},&quot;limits&quot;:{&quot;cpu&quot;:&quot;1&quot;}}}]}}&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Watch the kubelet dance‚Äîno container restart required!&lt;/p&gt;
&lt;h3 id=&quot;advanced-coderesizepolicycode-control&quot;&gt;3. Advanced &lt;code&gt;resizePolicy&lt;/code&gt; Control&lt;/h3&gt;
&lt;p&gt;Want CPU live-updates but memory restarts? Add a &lt;code&gt;resizePolicy&lt;/code&gt; section:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;resize-jedi&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;main&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;100m&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;128Mi&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;limits&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;200m&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;memory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;256Mi&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;resizePolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;resourceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cpu&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;restartPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NotRequired&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;# Live CPU tweaks!&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;resourceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;memory&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;restartPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RestartContainer&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Safe memory restarts&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;monitor-the-drama&quot;&gt;4. Monitor the Drama&lt;/h3&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe pod resize-jedi | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-A3&lt;/span&gt; PodResize
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Look for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PodResizePending&lt;/strong&gt;: ‚ÄúWe need to talk about resources‚Ä¶‚Äù&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PodResizeInProgress&lt;/strong&gt;: ‚ÄúHold tight‚Äîexpanding now!‚Äù&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this under-the-hood view, you now know exactly how Kubernetes pulls off its resource Jiu-Jitsu. Time to hit the dojo‚Äîer, cluster‚Äîand start bending pods to your will! ü•ãüöÄ&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;the-fine-print-limitations-because-nothings-perfect-&quot;&gt;The Fine Print: Limitations (Because Nothing‚Äôs Perfect) üìú&lt;/h2&gt;
&lt;p&gt;While this feature is cooler than a penguin in sunglasses, keep these in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;QoS Class Is Forever&lt;/strong&gt;&lt;br /&gt;
Your pod‚Äôs original Quality of Service class (&lt;code&gt;Guaranteed&lt;/code&gt; / &lt;code&gt;Burstable&lt;/code&gt; / &lt;code&gt;BestEffort&lt;/code&gt;) sticks like that tattoo you got in college. No upgrades from &lt;code&gt;BestEffort&lt;/code&gt; to &lt;code&gt;Guaranteed&lt;/code&gt; through resize alone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CPU &amp;amp; Memory Only (For Now)&lt;/strong&gt;&lt;br /&gt;
Want to hot-swap GPUs or tweak ephemeral storage on the fly? Dream on! Only CPU and memory can be resized in-place today.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory Shrinkage Requires Courage&lt;/strong&gt;&lt;br /&gt;
Decreasing memory limits without a container restart is like defusing a bomb‚Äîpossible in theory, but you‚Äôll want &lt;code&gt;restartPolicy: RestartContainer&lt;/code&gt; as your safety net, or chaos ensues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Not for Every Container&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Init &amp;amp; Ephemeral Containers&lt;/strong&gt;: Non-restartable init containers and ephemeral containers are off-limits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sidecars Welcome&lt;/strong&gt;: Good news if you love your little helpers‚Äîsidecar containers can be resized in-place!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No Resource Removal&lt;/strong&gt;&lt;br /&gt;
Once you set a request or limit, you can‚Äôt remove it via in-place resizing‚Äîonly change the values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Windows Users: Hold My Beer&lt;/strong&gt;&lt;br /&gt;
This party is Linux-only for now (&lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/&quot;&gt;Kubernetes docs confirm&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node-Level Exclusions&lt;/strong&gt;&lt;br /&gt;
Pods managed by static CPU or memory managers (e.g., &lt;code&gt;static&lt;/code&gt; CPU manager policy) are excluded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Swap Considerations&lt;/strong&gt;&lt;br /&gt;
If your pod uses swap, you can‚Äôt resize memory in-place unless you‚Äôve set &lt;code&gt;resizePolicy&lt;/code&gt; for memory to &lt;code&gt;RestartContainer&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;resizePolicy&lt;/code&gt; Is Set in Stone&lt;/strong&gt;&lt;br /&gt;
Once a pod is created, you can‚Äôt change its &lt;code&gt;resizePolicy&lt;/code&gt;. Choose wisely‚Äîthis is the ‚Äúforever‚Äù part of your resource romance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Gate May Be Required&lt;/strong&gt;&lt;br /&gt;
On some clusters, you‚Äôll need to enable &lt;code&gt;InPlacePodVerticalScaling=true&lt;/code&gt; in your apiserver, controller-manager, and scheduler.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Downsizing Pitfalls&lt;/strong&gt;&lt;br /&gt;
Trying to shrink memory below what‚Äôs currently in use (even with a &lt;code&gt;restartPolicy&lt;/code&gt;) can lead to &lt;code&gt;OOM&lt;/code&gt; surprises‚Äîbeware of under-provisioned chaos.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Keep these caveats in mind, and you‚Äôll avoid the worst of the drama. üé≠‚ú®&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;vpa-the-autoscaler-that-emcouldem-with-some-help-&quot;&gt;VPA: The Autoscaler That &lt;em&gt;Could&lt;/em&gt; (With Some Help) ü§ñ&lt;/h2&gt;
&lt;p&gt;As I &lt;a href=&quot;https://www.linkedin.com/pulse/vpa-kubernetes-autoscaler-could-doesnt-yet-alexei-ledenev-dkrbf&quot;&gt;recently lamented on LinkedIn&lt;/a&gt;, the Vertical Pod Autoscaler has been the awkward cousin at the scaling family reunion. But there&apos;s hope!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Future Integration We All Want:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;sequenceDiagram
    participant VPA
    participant K8s
    VPA-&amp;gt;&amp;gt;K8s: &amp;quot;Pod 42 needs more CPU!&amp;quot;
    K8s-&amp;gt;&amp;gt;Pod 42: Attempt in-place resize
    alt Success
        Pod 42--&amp;gt;&amp;gt;K8s: &amp;quot;Easy peasy!&amp;quot;
    else Failure
        K8s-&amp;gt;&amp;gt;VPA: &amp;quot;Resize failed :(&amp;quot;
        VPA-&amp;gt;&amp;gt;K8s: &amp;quot;Fine, recreate it then!&amp;quot;
    end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This hybrid approach (proposed in &lt;a href=&quot;https://github.com/kubernetes/enhancements/issues/4951&quot;&gt;KEP-4951&lt;/a&gt;) would make VPA finally production-ready for stateful workloads. Until then, we&apos;ll have to manually play resource Jenga with our pods.&lt;/p&gt;
&lt;h2 id=&quot;the-road-ahead-vertical-scalings-next-chapter-&quot;&gt;The Road Ahead: Vertical Scaling‚Äôs Next Chapter üöÄ&lt;/h2&gt;
&lt;p&gt;Kubernetes 1.33‚Äôs in-place pod resize is a giant leap toward making vertical scaling as seamless and non-disruptive as horizontal autoscaling‚Äîbut the story is far from over. As this feature matures, we‚Äôre already eyeing a true scaling renaissance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VPA Integration&lt;/strong&gt; (&lt;a href=&quot;https://github.com/kubernetes/autoscaler/issues/2534&quot;&gt;WIP&lt;/a&gt;)&lt;br /&gt;
Imagine a Vertical Pod Autoscaler that first attempts an in-place resize and falls back to recreate only when absolutely necessary‚Äîno more surprise pod evictions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-Resource Scaling&lt;/strong&gt;&lt;br /&gt;
Beyond CPU and memory, future releases may unlock GPUs, ephemeral storage, and more. Picture resizing your ML training pods on the fly, mid‚Äêjob!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scheduler Awareness&lt;/strong&gt;&lt;br /&gt;
Today, a resize doesn‚Äôt inform the scheduler‚Äîyour pod can still be evicted if node resources run low. Upcoming improvements could treat resized pods as first-class citizens, reserving headroom and avoiding unexpected re-scheduling.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Together, these enhancements promise a future where vertical scaling is truly dynamic, efficient, and interruption-free. So, go forth, Kubernetes adventurers‚Äîexperiment with in-place pod resize in your non-production clusters, share your findings, and help shape the next wave of scaling magic. Just don‚Äôt forget to read the fine print and test thoroughly before unleashing in production. Happy scaling! üöÄ&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Further Reading:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md&quot;&gt;KEP-1287: In-Place Pod Resizing Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kubernetes.io/blog/2025/04/23/kubernetes-v1-33-release/&quot;&gt;Official Kubernetes 1.33 Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/pulse/vpa-kubernetes-autoscaler-could-doesnt-yet-alexei-ledenev-dkrbf&quot;&gt;VPA: The Kubernetes Autoscaler That Could, But Doesn‚Äôt - Yet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Kubernetes" /><category term="kubernetes" /><category term="vpa" /><category term="devops" /><category term="autoscaling" /><summary type="html">Kubernetes 1.33: Resizing Pods Without the Drama (Finally!) üéâ</summary></entry><entry><title type="html">EKS GPU Cluster from Zero to Hero</title><link href="http://localhost:4000/development/2019/07/20/eks-gpu-spot.html" rel="alternate" type="text/html" title="EKS GPU Cluster from Zero to Hero" /><published>2019-07-20T11:00:00+03:00</published><updated>2019-07-20T11:00:00+03:00</updated><id>http://localhost:4000/development/2019/07/20/eks-gpu-spot</id><content type="html" xml:base="http://localhost:4000/development/2019/07/20/eks-gpu-spot.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;If you ever tried to run a GPU workload on Kubernetes cluster, you know that this task requires non-trivial configuration and comes with high cost tag (GPU instances are quite expensive).&lt;/p&gt;
&lt;p&gt;This post shows how to run a GPU workload on Kubernetes cluster in cost effective way, using &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;AWS EKS&lt;/a&gt; cluster, &lt;a href=&quot;https://aws.amazon.com/autoscaling/&quot;&gt;AWS Auto Scaling&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/ec2/spot/&quot;&gt;Amazon EC2 Spot Instances&lt;/a&gt; and some Kubernetes resources and configurations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/k8s_gpu.png&quot; alt=&quot;Kubernetes with GPU Mixed ASG&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;eks-cluster-plan&quot;&gt;EKS Cluster Plan&lt;/h2&gt;
&lt;p&gt;First we need to create a Kubernetes cluster that consists from mixed nodes, non-GPU nodes for management and generic Kubernetes workload and more expensive GPU nodes to run GPU intensive tasks, like machine learning, medical analysis, seismic exploration, video transcoding and others.&lt;/p&gt;
&lt;p&gt;These node groups should be able to scale on demand (scale out and scale in) for generic nodes, and from 0 to required number and back to 0 for expensive GPU instances. More than that, in order to do it in cost effective way, we are going to use  &lt;a href=&quot;https://aws.amazon.com/ec2/spot/&quot;&gt;Amazon EC2 Spot Instances&lt;/a&gt; both for generic nodes and GPU nodes.&lt;/p&gt;
&lt;h3 id=&quot;aws-ec2-spot-instances&quot;&gt;AWS EC2 Spot Instances&lt;/h3&gt;
&lt;p&gt;With &lt;a href=&quot;https://aws.amazon.com/ec2/spot/&quot;&gt;Amazon EC2 Spot Instances&lt;/a&gt; instances you can save up to 90% comparing to On-Demand price. Previously, Spot instances were terminated in ascending order of bids. The market prices fluctuated frequently because of this. In the current model, the Spot prices are more predictable, updated less frequently, and are determined by Amazon EC2 spare capacity, not bid prices. AWS EC2 service can reclaim SPot instances when there is not enough capacity for specific instance in specific Availability Zone. Spot instances receive a 2 minute alert when are about to be reclaimed by Amazon EC2 service and can use this time for graceful shutdown and state change.&lt;/p&gt;
&lt;h2 id=&quot;the-workflow&quot;&gt;The Workflow&lt;/h2&gt;
&lt;h3 id=&quot;create-eks-cluster&quot;&gt;Create EKS Cluster&lt;/h3&gt;
&lt;p&gt;It is possible to create &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;AWS EKS&lt;/a&gt; cluster, using &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/reference/eks&quot;&gt;AWS EKS CLI&lt;/a&gt;, CloudFormation or Terraform, &lt;a href=&quot;https://aws.amazon.com/cdk/&quot;&gt;AWS CDK&lt;/a&gt; or &lt;a href=&quot;https://eksctl.io&quot;&gt;eksctl&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&quot;eksctl-cli-tool&quot;&gt;eksctl CLI tool&lt;/h4&gt;
&lt;p&gt;In this post &lt;code&gt;eksctl&lt;/code&gt; (a CLI tool for creating clusters on EKS) is used.
It is possible to pass all parameters to the tool as CLI flags or configuration file. Using configuration file makes process more repeatable and automation friendly.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;eksctl&lt;/code&gt; can create or update EKS cluster and additional required AWS resources, using CloudFormation stacks.&lt;/p&gt;
&lt;p&gt;Customize your cluster by using a config file. Just run&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;eksctl create cluster &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to apply a cluster.yaml file:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;eksctl.io/v1alpha5&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterConfig&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test-cluster&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;us-west-2&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;nodeGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ng&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;instanceType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m5.large&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;desiredCapacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A new EKS cluster with 10 &lt;code&gt;m5.large&lt;/code&gt; On-Demand EC2 worker nodes will be created and cluster credentials will be added to &lt;code&gt;~/.kube/config&lt;/code&gt; file.&lt;/p&gt;
&lt;h3 id=&quot;creating-node-groups&quot;&gt;Creating node groups&lt;/h3&gt;
&lt;p&gt;As planned, we are going to create two node groups for Kubernetes worker nodes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;General&lt;/em&gt; node group - autoscaling group with Spot instances to run Kubernetes system workload and non-GPU workload&lt;/li&gt;
&lt;li&gt;&lt;em&gt;GPU&lt;/em&gt; node groups - autoscaling group with GPU-powered Spot Instances, that can scale from 0 to required number of instances and back to 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fortunately, the &lt;code&gt;eksctl&lt;/code&gt; supports adding Kubernetes node groups to EKS cluster and these groups can be composed from  Spot-only instances or mixture of Spot and On-Demand instances.&lt;/p&gt;
&lt;h4 id=&quot;general-node-group&quot;&gt;General node group&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;eksctl&lt;/code&gt; configuration file contains EKS cluster in &lt;code&gt;us-west-2&lt;/code&gt; across 3 Availability Zones and the first &lt;em&gt;General&lt;/em&gt; autoscaling (from 2 to 20 nodes) node group running on diversified Spot instances.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;eksctl.io/v1alpha5&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterConfig&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gaia-kube&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;us-west-2&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;availabilityZones&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2a&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2b&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;nodeGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# spot workers NG - multi AZ, scale from 3&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spot-ng&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ami&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;auto&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;instanceType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mixed&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;desiredCapacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;minSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;maxSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gp2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeEncrypted&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;iam&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;attachPolicyARNs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;withAddonPolicies&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;autoScaler&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ebs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;albIngress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cloudWatch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;instancesDistribution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;onDemandPercentageAboveBaseCapacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;instanceTypes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m4.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m4.4xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m5.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m5.4xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m5a.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;m5a.4xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;c4.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;c4.4xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;c5.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;c5.4xlarge&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;spotInstancePools&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s.io/cluster-autoscaler/enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;lifecycle&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ec2Spot&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;privateNetworking&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;availabilityZones&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2a&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2b&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;### next: GPU node groups ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now it is time to explain some parameters used in the above configuration file.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ami: auto&lt;/code&gt; - &lt;code&gt;eksctl&lt;/code&gt; automatically discover latest EKS-Optimized AMI image for worker nodes, based on specified AWS region, EKS version and instance type. See &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html&quot;&gt;Amazon EKS-Optimized AMI&lt;/a&gt; chapter in User Guide&lt;/li&gt;
&lt;li&gt;&lt;code&gt;instanceType: mixed&lt;/code&gt; - specify that actual instance type will be one of instance types defined in &lt;code&gt;instancesDistribution&lt;/code&gt; section&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iam&lt;/code&gt; contains list of predefined and in-place IAM policies; &lt;code&gt;eksctl&lt;/code&gt; creates a new &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html&quot;&gt;IAM Role&lt;/a&gt; with specified policies and attaches this role to every EKS worker node. There are several IAM policies you are required to attach to every EKS worker node, read &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/worker_node_IAM_role.html&quot;&gt;Amazon EKS Worker Node IAM Role&lt;/a&gt; section in User Guide and &lt;a href=&quot;https://eksctl.io/usage/iam-policies/&quot;&gt;&lt;code&gt;eksctl&lt;/code&gt; IAM policies&lt;/a&gt; documentation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;instancesDistribution&lt;/code&gt; - specify mixed instance policy for EC2 Auto Scaling Groups, read AWS &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_MixedInstancesPolicy.html&quot;&gt;MixedInstancesPolicy&lt;/a&gt; documentation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spotInstancePools&lt;/code&gt; - specifies number of Spot instance pools to use, &lt;a href=&quot;#Spot-Instance-Pools&quot;&gt;read more&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tags&lt;/code&gt; - AWS tags added to EKS worker nodes
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;k8s.io/cluster-autoscaler/enabled&lt;/code&gt; will use this tag for Kubernetes Cluster Autoscaler auto-discovery&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;privateNetworking: true&lt;/code&gt; - all EKS worker nodes will be placed into private subnets&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&quot;spot-instance-pools&quot;&gt;Spot Instance Pools&lt;/h5&gt;
&lt;p&gt;When you are using Spot instances as worker nodes you need to diversify usage to as many &lt;em&gt;Spot Instance pools&lt;/em&gt; as possible. A &lt;em&gt;Spot Instance pool&lt;/em&gt; is a set of unused EC2 instances with the same instance type (for example, &lt;code&gt;m5.large&lt;/code&gt;), operating system, Availability Zone, and network platforms.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;eksctl&lt;/code&gt; currently supports single Spot provisioning model: &lt;code&gt;lowestPrice&lt;/code&gt; allocation strategy. This strategy allows creation of a fleet of Spot Instances that is both cheap and diversified. Spot Fleet automatically deploys the cheapest combination of instance types and Availability Zones based on the current Spot price across the number of Spot pools that you specify. This combination allows avoiding the most expensive Spot Instances.&lt;/p&gt;
&lt;p&gt;The Spot instance diversification also increases worker nodes availability, typically not all &lt;em&gt;Spot Instance pools&lt;/em&gt; will be interrupted at the same time, so only a small portion of your workload will be interrupted and EC2 Auto-scaling group will replace interrupted instances from others &lt;em&gt;Spot Instance pools&lt;/em&gt;.&lt;/p&gt;
&lt;h4 id=&quot;gpu-powered-node-group&quot;&gt;GPU-powered node group&lt;/h4&gt;
&lt;p&gt;The next part of our &lt;code&gt;eksctl&lt;/code&gt; configuration file contains first &lt;em&gt;GPU&lt;/em&gt; autoscaling (from 0 to 10 nodes) node group running on diversified GPU-powered Spot instances.&lt;/p&gt;
&lt;p&gt;When using GPU-powered Spot instances, it&apos;s recommended to create &lt;em&gt;GPU&lt;/em&gt; node group per Availability Zone and configure Kubernetes Cluster Autoscaler to avoid automatic &lt;a href=&quot;https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html#arch-AutoScalingMultiAZ&quot;&gt;ASG rebalancing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Why is it important?
GPU-powered EC2 Spot Instances have relatively high &lt;em&gt;Frequency of interruption&lt;/em&gt; rate (&lt;code&gt;&amp;gt;20%&lt;/code&gt; for some GPU instance types) and using multiple AZ and disabling automatic Cluster Autoscaler balancing can help to minimize GPU workload interruptions.&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c1&quot;&gt;# ... EKS cluster and General node group ...&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# spot GPU NG - west-2a AZ, scale from 0&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gpu-spot-ng-a&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ami&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;auto&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;instanceType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mixed&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;desiredCapacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;minSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;maxSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeSize&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gp2&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeEncrypted&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;iam&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;attachPolicyARNs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;withAddonPolicies&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;autoScaler&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ebs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;fsx&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;efs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;albIngress&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;cloudWatch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;instancesDistribution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;onDemandPercentageAboveBaseCapacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;instanceTypes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.2xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.8xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.16xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.8xlarge&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.16xlarge&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;spotInstancePools&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tags&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s.io/cluster-autoscaler/node-template/taint/dedicated&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu=true&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s.io/cluster-autoscaler/node-template/label/nvidia.com/gpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&apos;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s.io/cluster-autoscaler/enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;lifecycle&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ec2Spot&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&apos;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s.amazonaws.com/accelerator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia-tesla&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;taints&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true:NoSchedule&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;privateNetworking&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;availabilityZones&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;us-west-2a&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# create additional node groups for other `us-west-2b` and `us-west-2c` availability zones ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now, it is time to explain some parameters used to configure GPU-powered node group.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ami: auto&lt;/code&gt; - &lt;code&gt;eksctl&lt;/code&gt; automatically discover latest EKS-Optimized AMI image with GPU support for worker nodes, based on specified AWS region, EKS version and instance type. See &lt;a href=&quot;https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html&quot;&gt;Amazon EKS-Optimized AMI with GPU support&lt;/a&gt; User Guide&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iam: withAddonPolicies&lt;/code&gt; - if a planned workload requires access to AWS storage services, it is important to include additional IAM policies (auto-generated by &lt;code&gt;eksctl&lt;/code&gt;)
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;efs: true&lt;/code&gt; - enable access to &lt;a href=&quot;https://aws.amazon.com/efs/&quot;&gt;Amazon EFS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fsx: true&lt;/code&gt; - enable access to &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/&quot;&gt;Amazon FSx for Lustre&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tags&lt;/code&gt; - AWS tags added to EKS worker nodes
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;k8s.io/cluster-autoscaler/node-template/taint/dedicated: nvidia.com/gpu=true&lt;/code&gt; - Kubernetes node taint&lt;/li&gt;
&lt;li&gt;&lt;code&gt;k8s.io/cluster-autoscaler/node-template/label/nvidia.com/gpu: &apos;true&apos;&lt;/code&gt; - Kubernetes node label used by Cluster Autoscaler to scale ASG from/to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;taints&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nvidia.com/gpu: &amp;quot;true:NoSchedule&amp;quot;&lt;/code&gt; - Kubernetes GPU node taint; helps to avoid placement on non-GPU workload on expensive GPU nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&quot;eks-optimized-ami-image-with-gpu-support&quot;&gt;EKS Optimized AMI image with GPU support&lt;/h5&gt;
&lt;p&gt;In addition to the standard Amazon EKS-optimized AMI configuration, the GPU AMI includes the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NVIDIA drivers&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;nvidia-docker2&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;nvidia-container-runtime&lt;/code&gt; (as the default runtime)&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&quot;scaling-a-node-group-tofrom-0&quot;&gt;Scaling a node group to/from 0&lt;/h5&gt;
&lt;p&gt;From Kubernetes &lt;a href=&quot;https://github.com/kubernetes/autoscaler/&quot;&gt;Cluster Autoscaler&lt;/a&gt; 0.6.1 - it is possible to scale a node group to/from 0, assuming that all scale-up and scale-down conditions are met.&lt;/p&gt;
&lt;p&gt;If you are using &lt;code&gt;nodeSelector&lt;/code&gt; you need to tag the ASG with a node-template key &lt;code&gt;k8s.io/cluster-autoscaler/node-template/label/&lt;/code&gt; and &lt;code&gt;k8s.io/cluster-autoscaler/node-template/taint/&lt;/code&gt; if you are using taints.&lt;/p&gt;
&lt;h4 id=&quot;scheduling-gpu-workload&quot;&gt;Scheduling GPU workload&lt;/h4&gt;
&lt;h5 id=&quot;schedule-based-on-gpu-resources&quot;&gt;Schedule based on GPU resources&lt;/h5&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/NVIDIA/k8s-device-plugin&quot;&gt;NVIDIA device plugin for Kubernetes&lt;/a&gt; exposes the number of GPUs on each nodes of your cluster. Once the plugin is installed, it&apos;s possible to use &lt;code&gt;nvidia/gpu&lt;/code&gt; Kubernetes resource on GPU nodes and for Kubernetes workloads.&lt;/p&gt;
&lt;p&gt;Run this command to apply the Nvidia Kubernetes device plugin as a &lt;code&gt;daemonset&lt;/code&gt; running only on AWS GPU-powered worker nodes, using &lt;code&gt;tolerations&lt;/code&gt; and &lt;code&gt;nodeAffinity&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; kubernetes/nvidia-device-plugin.yaml

kubectl get daemonset &lt;span class=&quot;nt&quot;&gt;-nkube-system&lt;/span&gt;

NAME                                  DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
aws-node                              5         5         5       5            5           &amp;lt;none&amp;gt;          8d
kube-proxy                            5         5         5       5            5           &amp;lt;none&amp;gt;          8d
nvidia-device-plugin-daemonset-1.12   3         3         3       3            3           &amp;lt;none&amp;gt;          8d
ssm-agent                             5         5         5       5            5           &amp;lt;none&amp;gt;          8d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;using &lt;code&gt;nvidia-device-plugin.yaml&lt;/code&gt; Kubernetes resource file&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DaemonSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia-device-plugin-daemonset-1.12&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;updateStrategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RollingUpdate&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia-device-plugin-ds&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Exists&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;effect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NoSchedule&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia/k8s-device-plugin:1.11&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nvidia-device-plugin-ctr&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;securityContext&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;allowPrivilegeEscalation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;capabilities&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ALL&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;device-plugin&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/var/lib/kubelet/device-plugins&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;device-plugin&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/var/lib/kubelet/device-plugins&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;affinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;nodeAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;nodeSelectorTerms&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;beta.kubernetes.io/instance-type&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.2xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.8xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3.16xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p3dn.24xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.8xlarge&lt;/span&gt;
                &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;p2.16xlarge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;taints-and-tolerations&quot;&gt;Taints and Tolerations&lt;/h5&gt;
&lt;p&gt;Kubernetes &lt;em&gt;taints&lt;/em&gt;  allow a node to repel a set of pods. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.&lt;/p&gt;
&lt;p&gt;See Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/&quot;&gt;Taints and Tolerations&lt;/a&gt; documentation for more details.&lt;/p&gt;
&lt;p&gt;In order to run GPU workload to run on GPU-powered Spot instance nodes, with &lt;code&gt;nvidia.com/gpu: &amp;quot;true:NoSchedule&amp;quot;&lt;/code&gt; taint, the workload must include both matching &lt;code&gt;tolerations&lt;/code&gt; and &lt;code&gt;nodeSelector&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Kubernetes deployment with &lt;code&gt;10&lt;/code&gt; pod replicas with &lt;code&gt;nvidia/gpu: 1&lt;/code&gt; limit:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;nodeSelector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Exists&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;effect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NoSchedule&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cuda-vector-add&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;# https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;k8s.gcr.io/cuda-vector-add:v0.1&quot;&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;limits&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;nvidia.com/gpu&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# requesting 1 GPU&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Deploy &lt;code&gt;cuda-vector-add&lt;/code&gt; deployment and see how new GPU-powered nodes are added to the EKS cluster.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# list Kubernetes nodes before running GPU workload&lt;/span&gt;
NAME                                            ID                                      TYPE
ip-192-168-151-104.us-west-2.compute.internal   aws:///us-west-2b/i-01d4c83eaee18b7b3   c4.4xlarge
ip-192-168-171-140.us-west-2.compute.internal   aws:///us-west-2c/i-07ec09fd128e1393f   c4.4xlarge


&lt;span class=&quot;c&quot;&gt;# deploy GPU workload on EKS cluster with tolerations for nvidia/gpu=true&lt;/span&gt;
kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; kubernetes/examples/vector/vector-add-dpl.yaml

&lt;span class=&quot;c&quot;&gt;# list Kubernetes nodes after several minutes to see new GPU nodes added to the cluster&lt;/span&gt;
kubectl get nodes &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;custom-columns=NAME:.metadata.name,ID:.spec.providerID,TYPE:.metadata.labels.beta&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;kubernetes&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\/&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;instance-type&quot;&lt;/span&gt;

NAME                                            ID                                      TYPE
ip-192-168-101-60.us-west-2.compute.internal    aws:///us-west-2a/i-037d1994fe96eeffc   p2.16xlarge
ip-192-168-139-227.us-west-2.compute.internal   aws:///us-west-2b/i-0038eb8d2c795fb40   p2.16xlarge
ip-192-168-151-104.us-west-2.compute.internal   aws:///us-west-2b/i-01d4c83eaee18b7b3   c4.4xlarge
ip-192-168-171-140.us-west-2.compute.internal   aws:///us-west-2c/i-07ec09fd128e1393f   c4.4xlarge
ip-192-168-179-248.us-west-2.compute.internal   aws:///us-west-2c/i-0bc0853ef26c0c054   p2.16xlarge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As you can see, 3 new GPU-powered nodes (&lt;code&gt;p2.16xlarge&lt;/code&gt;), across 3 AZ, had been added to the cluster. When you delete GPU workload, the cluster will scale down GPU node group to 0 after 10 minutes.&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Follow this tutorial to create an EKS (Kubernetes) cluster with GPU-powered node group, running on Spot instances and scalable from/to 0 nodes.&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/alexei-led/eks-spot-cluster&quot;&gt;EKS Spot Cluster&lt;/a&gt; GitHub repository with code for this blog&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://itnext.io/the-definitive-guide-to-running-ec2-spot-instances-as-kubernetes-worker-nodes-68ef2095e767&quot;&gt;The definitive guide to running EC2 Spot Instances as Kubernetes worker nodes&lt;/a&gt; by Ran Sheinberg&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler&quot;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/&quot;&gt;Taints and Tolleratoins&lt;/a&gt; Kubernetes documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;It does not matter where I work, all my opinions are my own.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Development" /><category term="Kubernetes" /><category term="EKS" /><category term="GPU" /><category term="Spot" /><category term="AWS" /><category term="Machine Learning" /><summary type="html">Introduction</summary></entry><entry><title type="html">Kubernetes Continuous Integration</title><link href="http://localhost:4000/development/2019/03/08/kubernetes-continuous-integration.html" rel="alternate" type="text/html" title="Kubernetes Continuous Integration" /><published>2019-03-08T10:00:00+02:00</published><updated>2019-03-08T10:00:00+02:00</updated><id>http://localhost:4000/development/2019/03/08/kubernetes-continuous-integration</id><content type="html" xml:base="http://localhost:4000/development/2019/03/08/kubernetes-continuous-integration.html">&lt;h1 id=&quot;kubernetes-configuration-as-code&quot;&gt;Kubernetes configuration as Code&lt;/h1&gt;
&lt;p&gt;Complex Kubernetes application consists from multiple Kubernetes resources, defined in YAML files. Authoring a properly formatted YAML files that are also a valid Kubernetes specification, that should also comply to some policy can be a challenging task.&lt;/p&gt;
&lt;p&gt;These YAML files are your application deployment and configuration code and should be addressed as code.&lt;/p&gt;
&lt;p&gt;As with code, Continuous Integration approach should be applied to a Kubernetes configuration files.&lt;/p&gt;
&lt;h2 id=&quot;git-repository&quot;&gt;Git Repository&lt;/h2&gt;
&lt;p&gt;Create a separate Git repository that contains Kubernetes configuration files. Define a Continuous Integration pipeline that is triggered automatically for for every change and can validate it without human intervention.&lt;/p&gt;
&lt;h2 id=&quot;helm&quot;&gt;Helm&lt;/h2&gt;
&lt;p&gt;Helm helps you manage complex Kubernetes applications. Using Helm Charts you define, install, test and upgrade Kubernetes application.&lt;/p&gt;
&lt;p&gt;Here I&apos;m going to focus on using Helm for authoring complex Kubernetes application.&lt;/p&gt;
&lt;p&gt;The same Kubernetes application can be installed in multiple environments: development, testing, staging and production. Helm template helps to separate application structure from environment configuration by keeping environment specific values in external files.&lt;/p&gt;
&lt;h3 id=&quot;dependency-management&quot;&gt;Dependency Management&lt;/h3&gt;
&lt;p&gt;Helm also helps with dependency management. A typical Kubernetes application can be composed from multiple services developed by other teams and open source community.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;requirements.yaml&lt;/code&gt; file is a YAML file in which developers can declare Helm chart dependencies, along with the location of the chart and the desired version. For example, this requirements file declares two dependencies:&lt;/p&gt;
&lt;p&gt;Where possible, use version ranges instead of pinning to an exact version. The suggested default is to use a patch-level version match:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;version: ~1.2.3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will match version 1.2.3 and any patches to that release. In other words, ~1.2.3 is equivalent to &amp;gt;= 1.2.3, &amp;lt; 1.3.0&lt;/p&gt;
&lt;h2 id=&quot;yaml&quot;&gt;YAML&lt;/h2&gt;
&lt;p&gt;YAML is the most convenient way to write Kubernetes configuration files. YAML is easier for humans to read and write than other common data formats like XML or JSON. Still it&apos;s recommended to use automatic YAML linters to avoid syntax and formatting errors.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/adrienverge/yamllint&quot;&gt;yamlint&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;helm-chart-validation&quot;&gt;Helm Chart Validation&lt;/h3&gt;
&lt;p&gt;Helm has a &lt;code&gt;helm lint&lt;/code&gt; command that runs a series of tests to verify that the chart is well-formed. The &lt;code&gt;helm lint&lt;/code&gt; also converts YAML to JSON, and this ways is able to detect some YAML errors.&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm lint mychart

==&amp;gt; Linting mychart
[ERROR] templates/deployment.yaml: unable to parse YAML
    error converting YAML to JSON: yaml: line 53: did not find expected &apos;-&apos; indicator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;There are few issues with &lt;code&gt;helm lint&lt;/code&gt;, you should be aware of:
- no real YAML validation is done: only YAML to JSON conversion errors are detected
- it shows wrong error line number: no the actual line in a template that contains the detected error&lt;/p&gt;
&lt;p&gt;So, I recommend also to use YAML linter, like &lt;code&gt;yamllint&lt;/code&gt; to perform YAML validation.&lt;/p&gt;
&lt;p&gt;First, you need to generate Kubernetes YAML files from a Helm chart. The &lt;code&gt;helm template&lt;/code&gt; renders chart templates locally and prints the output to the &lt;code&gt;stdout&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm template &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--values&lt;/span&gt; dev-values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Pipe &lt;code&gt;helm template&lt;/code&gt; and &lt;code&gt;yamllint&lt;/code&gt; together to validate rendered YAML files.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm template mychart | yamllint -

stdin
  41:81     error    line too long &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;93 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 80 characters&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;line-length&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  43:1      error    trailing spaces  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;trailing-spaces&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  151:9     error    wrong indentation: expected 10 but found 8  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;indentation&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  245:10    error    syntax error: expected &amp;lt;block end&amp;gt;, but found &lt;span class=&quot;s1&quot;&gt;&apos;&amp;lt;block sequence start&amp;gt;&apos;&lt;/span&gt;
  293:1     error    too many blank lines &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1 &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; 0&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;empty-lines&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now there are multiple ways to inspect these errors:&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# using vim editor&lt;/span&gt;
vim &amp;lt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;helm template mychart&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# using cat command (with line number)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; &amp;lt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;helm template mychart&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# printing error line and few lines around it, replacing spaces with dots&lt;/span&gt;
helm template hermes | &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 240,250p | &lt;span class=&quot;nb&quot;&gt;tr&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos; &apos;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;‚ãÖ&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;valid-kubernetes-configuration&quot;&gt;Valid Kubernetes Configuration&lt;/h2&gt;
&lt;p&gt;When authoring Kubernetes configuration files, it&apos;s important not only check if they are valid YAML files, but if they are valid Kubernetes files.&lt;/p&gt;
&lt;p&gt;It turns out that the Kubernetes supports OpenAPI specification and it&apos;s possible to generate Kubernetes JSON schema for every Kubernetes API version.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/garethr&quot;&gt;Gareth Rushgrove&lt;/a&gt; wrote a &lt;a href=&quot;https://www.morethanseven.net/2017/06/26/schemas-for-kubernetes-types/&quot;&gt;blog post&lt;/a&gt; on this topic and maintains the &lt;a href=&quot;https://github.com/garethr/kubernetes-json-schema&quot;&gt;garethr/kubernetes-json-schema&lt;/a&gt; GitHub repository with most recent Kubernetes and OpenShift JSON schemas for all API versions. What a great contribution to the community!&lt;/p&gt;
&lt;p&gt;Now, with Kubernetes API JSON schema, it&apos;s possible to validate any YAML file if it&apos;s a valid Kubernetes configuration file.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/garethr/kubeval&quot;&gt;kubeval&lt;/a&gt; tool (also authored by Gareth Rushgrove) is to help.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;kubeval&lt;/code&gt; piped with &lt;code&gt;helm template&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm template mychart | kubeval
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The output below sows single detected error in &lt;code&gt;Service&lt;/code&gt; definition: invalid annotation. The &lt;code&gt;kubeval&lt;/code&gt; could be more specific, by providing &lt;code&gt;Service&lt;/code&gt; name, but event AS IS it is a valuable output for detecting Kubernetes configuration errors.&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The document stdin contains a valid Secret
The document stdin contains a valid Secret
The document stdin contains a valid ConfigMap
The document stdin contains a valid ConfigMap
The document stdin contains a valid PersistentVolumeClaim
The document stdin contains an invalid Service
---&amp;gt; metadata.annotations: Invalid type. Expected: object, given: null
The document stdin contains a valid Service
The document stdin contains a valid Deployment
The document stdin contains a valid Deployment
The document stdin is empty
The document stdin is empty
The document stdin is empty
The document stdin is empty
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/garethr/kubetest&quot;&gt;https://github.com/garethr/kubetest&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Development" /><category term="Kubernetes" /><category term="CI/CD" /><category term="Continuous Integration" /><category term="Helm" /><summary type="html">Kubernetes configuration as Code</summary></entry><entry><title type="html">Chaos Testing for Docker Containers</title><link href="http://localhost:4000/chaos%20testing/2017/10/04/pumba-containercamp.html" rel="alternate" type="text/html" title="Chaos Testing for Docker Containers" /><published>2017-10-04T19:00:00+03:00</published><updated>2017-10-04T19:00:00+03:00</updated><id>http://localhost:4000/chaos%20testing/2017/10/04/pumba-containercamp</id><content type="html" xml:base="http://localhost:4000/chaos%20testing/2017/10/04/pumba-containercamp.html">&lt;p&gt;What follows is the text of my presentation, &lt;em&gt;Chaos Testing for Docker Containers&lt;/em&gt; that I gave at &lt;a href=&quot;https://2017.container.camp/uk&quot;&gt;ContainerCamp&lt;/a&gt; in London this year.
I&apos;ve also decided to turn the presentation into an article. I edited the text slightly for readability and added some links for more context. You can find the original video recording and slides at the end of this post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/sink_cargo.jpg&quot; alt=&quot;Docker Chaos Testing&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;
&lt;p&gt;Software development is about building software services that support business needs. More complex businesses processes we want to automate and integrate with. the more complex software system we are building. And solution complexity is tend to grown over time and scope.&lt;/p&gt;
&lt;p&gt;The reasons for growing complexity can be different. Some systems just tend to handle too many concerns, or require a lot of integrations with external services and internal legacy systems. These systems are written and rewritten multiple times over several years by different people with different skills, trying to satisfy constantly changing business requirements, using different technologies, following different technology and architecture trends.&lt;/p&gt;
&lt;p&gt;So, my point, is that building software, that unintentionally become more and more complex over time, is easy - we all done in the past it or doing it right now. Building a &amp;quot;good&amp;quot; software architecture for complex systems and preserving it&apos;s &amp;quot;good&amp;quot; abilities for some period of time, is really hard.&lt;/p&gt;
&lt;p&gt;When you have too many &amp;quot;moving&amp;quot; parts, integrations, constantly changing requirements, that lead to code changes, security upgrades, hardware modernization, multiple network communication channels and etc, it can become a &amp;quot;Mission Impossible&amp;quot; to avoid unexpected failures.&lt;/p&gt;
&lt;h2 id=&quot;stuff-happens&quot;&gt;Stuff happens!&lt;/h2&gt;
&lt;p&gt;All systems fail from time to time. And your software system will fail too. Take this as a fact of life. There will always be something that can‚Ää‚Äî‚Ääand will‚Ää‚Äî‚Äägo wrong. No matter how hard we try, we can‚Äôt build perfect software, nor can the companies we depend on. Even the most stable and respectful services from companies, that practice CI/CD, test driven development (TDD/BDD), have huge QA departments and well defined release procedures, fail.&lt;/p&gt;
&lt;p&gt;Just a few examples from the last year outages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;BM, January 26&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;IBM&apos;s cloud credibility took a hit at the start of the year when a management portal used by customers to access its Bluemix cloud infrastructure went down for several hours. While no underlying infrastructure actually failed, users were frustrated in finding they couldn&apos;t manage their applications or add or remove cloud resources powering workloads.&lt;/li&gt;
&lt;li&gt;IBM said the problem was intermittent and stemmed from a botched update to the interface.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitLab, January 31&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GitLab&apos;s popular online code repository, GibLab.com, suffered an 18-hour service outage that ultimately couldn&apos;t be fully remediated.&lt;/li&gt;
&lt;li&gt;The problem resulted when an employee removed a database directory from the wrong database server during maintenance procedures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AWS, February 28&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.crn.com/news/cloud/300083958/aws-storage-outage-wreaking-havoc-on-web-services-providers.htm&quot;&gt;This was the outage&lt;/a&gt; that shook the industry.&lt;/li&gt;
&lt;li&gt;An Amazon Web Services engineer trying to debug an S3 storage system in the provider&apos;s Virginia data center accidentally typed a command incorrectly, and much of the Internet ‚Äì including many enterprise platforms like Slack, Quora and Trello ‚Äì was down for four hours.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Microsoft Azure, March 16&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Storage availability issues plagued Microsoft&apos;s Azure public cloud for more than eight hours, mostly affecting customers in the Eastern U.S.&lt;/li&gt;
&lt;li&gt;Some users had trouble provisioning new storage or accessing existing resources in the region. A Microsoft engineering team later identified the culprit as a storage cluster that lost power and became unavailable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Visit &lt;a href=&quot;http://outage.report&quot;&gt;Outage.Report&lt;/a&gt; or &lt;a href=&quot;http://downdetector.com&quot;&gt;Downdetector&lt;/a&gt; to see a constantly updating long list of outages reported by end-users.&lt;/p&gt;
&lt;h2 id=&quot;chasing-software-quality&quot;&gt;Chasing Software Quality&lt;/h2&gt;
&lt;p&gt;As software engineers, we what to be proud of software systems we are building. We want theses systems to be of high quality, without functional bugs, security holes, providing exceptional performance, resilient to unexpected failures, self-healing, always available and easy to maintain and modernize.&lt;/p&gt;
&lt;p&gt;Every new project starts with &amp;quot;high quality&amp;quot; picture in mind and none wants to create crappy software, but very few of us (or none) are able to achieve and keep intact all good &amp;quot;abilities&amp;quot;. So, what we can do to improve overall system quality? Should we do more testing?&lt;/p&gt;
&lt;p&gt;I tend to say &amp;quot;Yes&amp;quot; - software testing is critical. But just running unit, functional and performance testing is not enough.&lt;/p&gt;
&lt;p&gt;Today, building complex distributed system is much easier with all new amazing technology we have and experience we gathered. Microservice Architecture is a real trend nowadays and miscellaneous container technologies support this architecture. It&apos;s much easier to deploy, scale, link, monitor, update and manage distributed systems, composed from multiple &amp;quot;microservices&amp;quot;.
When we are building distributed systems, we are choosing &lt;strong&gt;P&lt;/strong&gt; (&lt;em&gt;Pratition Tolerance&lt;/em&gt;) from the &lt;a href=&quot;https://en.wikipedia.org/wiki/CAP_theorem&quot;&gt;CAP theorem&lt;/a&gt; and second to it either &lt;strong&gt;A&lt;/strong&gt; (&lt;em&gt;Availability&lt;/em&gt; - the most popular choice) or &lt;strong&gt;C&lt;/strong&gt; (&lt;em&gt;Consistency&lt;/em&gt;). So, we need to find a good approach for testing &lt;strong&gt;AP&lt;/strong&gt; or &lt;strong&gt;CP&lt;/strong&gt; systems.&lt;/p&gt;
&lt;p&gt;Traditional testing disciplines and tools do not provide a good answer to &lt;em&gt;how does your distributed system behave when unexpected stuff happens in production?&lt;/em&gt;.
Sure, you can learn from previous failures, after the fact, and you should definitely do it. But, learning from past experience should not be the only way to prepare for the future failures.&lt;/p&gt;
&lt;p&gt;Waiting for things to break in production is not an option. But what‚Äôs the alternative?&lt;/p&gt;
&lt;h2 id=&quot;chaos-engineering&quot;&gt;Chaos Engineering&lt;/h2&gt;
&lt;p&gt;The alternative is to break things on purpose. And Chaos Engineering is a particular approach to doing just that. The idea of Chaos Engineering is to &lt;em&gt;embrace the failure!&lt;/em&gt;
Chaos Engineering for distributed software systems was originally popularized by Netflix.&lt;/p&gt;
&lt;p&gt;Chaos Engineering defines an empirical approach to resilience testing of distributed software systems. You are testing a system by conducting &lt;em&gt;chaos experiments&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Typical &lt;em&gt;chaos experiment&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define a &lt;em&gt;normal/steady&lt;/em&gt; state of the system (e.g. by monitoring a set of system and business metrics)&lt;/li&gt;
&lt;li&gt;pseudo-randomly inject faults (e.g. by terminating VMs, killing containers or changing network behavior)&lt;/li&gt;
&lt;li&gt;try to discover system weaknesses by deviation from expected or steady-state behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The harder it is to disrupt the steady state, the more confidence we have in the behavior of the system. ¬†&lt;/p&gt;
&lt;h2 id=&quot;chaos-engineering-tools&quot;&gt;Chaos Engineering tools&lt;/h2&gt;
&lt;p&gt;Of cause it&apos;s possible to practice Chaos Engineering manually, or relay on automatic system updates, but we, as engineers like to automate boring manual tasks, so there are some nice tools to use.&lt;/p&gt;
&lt;p&gt;Netflix built a some &lt;a href=&quot;https://github.com/Netflix/SimianArmy/wiki&quot;&gt;useful tools&lt;/a&gt; for practicing Chaos Engineering in public cloud (AWS):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chaos Monkey - kill EC2, kill processes, burn CPU, fill disk, detach volumes, add network latency, etc&lt;/li&gt;
&lt;li&gt;Chaos Kong - remove whole AWS Regions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are very good tools, I encourage you to use them. But when I&apos;ve started my new container based project (2 years ago), it looks like these tools provided just a &lt;em&gt;wrong&lt;/em&gt; granularity for &lt;em&gt;chaos&lt;/em&gt; I wanted to create, and I wanted to be able to create the &lt;em&gt;chaos&lt;/em&gt; not only in real cluster, but also on single developer machine, to be able to debug and tune my application. So, I&apos;ve searched Google for &lt;em&gt;Chaos Monkey for Docker&lt;/em&gt;, but did not find anything, besides some basic Bash scripts.
So, I&apos;ve decided to create my own tool. And since it happens to be quite a useful tool from the very first version, I&apos;ve shared it with a community as an open source. It&apos;s a Chaos &lt;del&gt;Monkey&lt;/del&gt; Warthog for Docker - &lt;a href=&quot;https://github.com/gaia-adm/pumba&quot;&gt;Pumba&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;pumba---chaos-testing-for-docker&quot;&gt;Pumba - Chaos Testing for Docker&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;What is Pumba(a)?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Those of us who have kids or was a kid in 90s should remember this character from Disney&apos;s animated film &lt;strong&gt;The Lion King&lt;/strong&gt;. In Swahili,¬†&lt;strong&gt;pumbaa&lt;/strong&gt;¬†means &amp;quot;&lt;em&gt;to be foolish, silly, weak-minded, careless, negligent&lt;/em&gt;&amp;quot;. I like the Swahili meaning of this word. It matched perfectly for the tool I wanted to create.&lt;/p&gt;
&lt;h3 id=&quot;what-pumba-can-do&quot;&gt;What Pumba can do?&lt;/h3&gt;
&lt;p&gt;Pumba disturbs running Docker runtime environment by injecting different failures. Pumba can &lt;code&gt;kill&lt;/code&gt;, &lt;code&gt;stop&lt;/code&gt;, &lt;code&gt;remove&lt;/code&gt; or &lt;code&gt;pause&lt;/code&gt; Docker container.
Pumba can also do a network emulation, simulating different network failures, like: delay, packet loss (using different probability loss models), bandwidth rate limits and more. For network emulation, Pumba uses Linux kernel traffic control &lt;code&gt;tc&lt;/code&gt; with¬†&lt;code&gt;netem&lt;/code&gt;¬†queueing discipline, read more &lt;a href=&quot;http://man7.org/linux/man-pages/man8/tc-netem.8.html&quot;&gt;here&lt;/a&gt;. If &lt;code&gt;tc&lt;/code&gt; is not available within target container, Pumba uses a &lt;em&gt;sidekick&lt;/em&gt; container with &lt;code&gt;tc&lt;/code&gt; on-board, attaching it to the target container network.&lt;/p&gt;
&lt;p&gt;You can pass list of containers to the Pumba or just write a regular expression to select matching containers. If you will not specify containers, Pumba will try to disturb all running containers. Use &lt;code&gt;--random&lt;/code&gt; option, to randomly select only one target container from the provided list. It&apos;s also possible to define a repeatable time interval and duration parameters to better control the amount of &lt;em&gt;chaos&lt;/em&gt; you want to create.&lt;/p&gt;
&lt;p&gt;Pumba is available as a single binary file for Linux, MacOS and Windows, or as a Docker container.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Download binary from https://github.com/gaia-adm/pumba/releases&lt;/span&gt;
curl https://github.com/gaia-adm/pumba/releases/download/0.4.6/pumba_linux_amd64 &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; /usr/local/bin/pumba
&lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x /usr/local/bin/pumba &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; pumba &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install with Homebrew (MacOS only)&lt;/span&gt;
brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;pumba &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; pumba &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Use Docker image&lt;/span&gt;
docker run gaiaadm/pumba pumba &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;pumba-commands-examples&quot;&gt;Pumba commands examples&lt;/h3&gt;
&lt;p&gt;First of all, run &lt;code&gt;pumba --help&lt;/code&gt; to get help about available commands and options and &lt;code&gt;pumba &amp;lt;command&amp;gt; --help&lt;/code&gt; to get help for the specific command and sub-command.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# pumba help&lt;/span&gt;
pumba &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pumba kill help&lt;/span&gt;
pumba &lt;span class=&quot;nb&quot;&gt;kill&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# pumba netem delay help&lt;/span&gt;
pumba netem delay &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Killing randomly chosen Docker container from  &lt;code&gt;^test&lt;/code&gt; regex list.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# on main pane/screen, run 7 test containers that do nothing&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;i &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;0..7&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt; alpine &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; /dev/null&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# run an additional container with &apos;skipme&apos; name&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; skipme alpine &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; /dev/null

&lt;span class=&quot;c&quot;&gt;# run this command in another pane/screen to see running docker containers&lt;/span&gt;
watch docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# go back to main pane/screen and kill (once in 10s) random &apos;test&apos; container, ignoring &apos;skipme&apos;&lt;/span&gt;
pumba &lt;span class=&quot;nt&quot;&gt;--random&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--interval&lt;/span&gt; 10s &lt;span class=&quot;nb&quot;&gt;kill &lt;/span&gt;re2:^test
&lt;span class=&quot;c&quot;&gt;# press Ctrl-C to stop Pumba at any time&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Adding a &lt;code&gt;3000ms&lt;/code&gt; (&lt;code&gt;+-50ms&lt;/code&gt;) delay to the &lt;em&gt;engress&lt;/em&gt; traffic for the &lt;code&gt;ping&lt;/code&gt; container for &lt;code&gt;20&lt;/code&gt; seconds, using &lt;em&gt;normal&lt;/em&gt; distribution model.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# run &quot;ping&quot; container on one screen/pane&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ping alpine ping 8.8.8.8

&lt;span class=&quot;c&quot;&gt;# on second screen/pane, run pumba netem delay command, disturbing &quot;ping&quot; container; sidekick a &quot;tc&quot; helper container&lt;/span&gt;
pumba netem &lt;span class=&quot;nt&quot;&gt;--duration&lt;/span&gt; 20s &lt;span class=&quot;nt&quot;&gt;--tc-image&lt;/span&gt; gaiadocker/iproute2 delay &lt;span class=&quot;nt&quot;&gt;--time&lt;/span&gt; 3000 jitter 50 &lt;span class=&quot;nt&quot;&gt;--distribution&lt;/span&gt; normal ping
&lt;span class=&quot;c&quot;&gt;# pumba will exit after 20s, or stop it with Ctrl-C&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To demonstrate packet loss capability, we will need three screens/panes. I will use &lt;code&gt;iperf&lt;/code&gt; network bandwidth measurement &lt;a href=&quot;https://iperf.fr&quot;&gt;tool&lt;/a&gt;.
On the first pane, run &lt;em&gt;server&lt;/em&gt; docker container with &lt;code&gt;iperf&lt;/code&gt; on-board and start there a UDP server. On the second pane, start &lt;em&gt;client&lt;/em&gt; docker container with &lt;code&gt;iperf&lt;/code&gt; and send datagrams to the &lt;em&gt;server&lt;/em&gt; container. Then, on the third pane, run &lt;code&gt;pumba netem loss&lt;/code&gt; command, adding a packet loss to the &lt;em&gt;client&lt;/em&gt; container. Enjoy the chaos.&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create docker network&lt;/span&gt;
docker network create &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; bridge testnet

&lt;span class=&quot;c&quot;&gt;# &amp;gt; Server Pane&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# run server container&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; server &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt; testnet &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; alpine sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;apk add --no-cache iperf; sh&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# shell inside server container: run a UDP Server listening on UDP port 5001&lt;/span&gt;
sh&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;iperf &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; 1

&lt;span class=&quot;c&quot;&gt;# &amp;gt; Client Pane&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# run client container&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; client &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt; testnet &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; alpine sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;apk add --no-cache iperf; sh&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# shell inside client container: send datagrams to the server -&amp;gt; see no packet loss&lt;/span&gt;
sh&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;iperf &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; server &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# &amp;gt; Server Pane&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# see server receives datagrams without any packet loss&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# &amp;gt; Pumba Pane&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# inject 20% packet loss into client container, for 1m&lt;/span&gt;
pumba netem &lt;span class=&quot;nt&quot;&gt;--duration&lt;/span&gt; 1m &lt;span class=&quot;nt&quot;&gt;--tc-image&lt;/span&gt; gaiadocker/iproute2 loss &lt;span class=&quot;nt&quot;&gt;--percent&lt;/span&gt; 20 client

&lt;span class=&quot;c&quot;&gt;# &amp;gt; Client Pane&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# shell inside client container: send datagrams to the server -&amp;gt; see ~20% packet loss&lt;/span&gt;
sh&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;iperf &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; server &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;session-and-slides&quot;&gt;Session and slides&lt;/h2&gt;
&lt;div class=&quot;embed-container&quot;&gt;
  &amp;lt;iframe
      src=&quot;https://www.youtube.com/embed/68ZepHa5UVg&quot;
      width=&quot;700&quot;
      height=&quot;480&quot;
      frameborder=&quot;0&quot;
      allowfullscreen=&quot;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://speakerdeck.com/alexeiled/chaos-testing-for-docker-containers&quot;&gt;Slides&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Hope, you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Chaos Testing" /><category term="docker" /><category term="chaos monkey" /><category term="chaos testing" /><category term="chaos" /><category term="testing" /><category term="devops" /><category term="chaos engineering" /><category term="netem" /><category term="network emulation" /><summary type="html">What follows is the text of my presentation, Chaos Testing for Docker Containers that I gave at ContainerCamp in London this year. I&apos;ve also decided to turn the presentation into an article. I edited the text slightly for readability and added some links for more context. You can find the original video recording and slides at the end of this post.</summary></entry><entry><title type="html">Debugging remote Node.js application running in a Docker container</title><link href="http://localhost:4000/docker/2017/06/03/debug-node-in-docker.html" rel="alternate" type="text/html" title="Debugging remote Node.js application running in a Docker container" /><published>2017-06-03T19:00:00+03:00</published><updated>2017-06-03T19:00:00+03:00</updated><id>http://localhost:4000/docker/2017/06/03/debug-node-in-docker</id><content type="html" xml:base="http://localhost:4000/docker/2017/06/03/debug-node-in-docker.html">&lt;h2 id=&quot;teaser&quot;&gt;Teaser&lt;/h2&gt;
&lt;p&gt;Suppose you want to debug a Node.js application already running on a remote machine inside Docker container. And would like to do it without modifying command arguments (enabling &lt;code&gt;debug&lt;/code&gt; mode) and opening remote Node.js debugger agent port to the whole world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I bet you didn&apos;t know that it&apos;s possible and also have no idea how to do it.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I encourage you to continue reading this post if you are eager to learn some new cool stuff.&lt;/p&gt;
&lt;h2 id=&quot;the-todomvc-demo-application&quot;&gt;The TodoMVC demo application&lt;/h2&gt;
&lt;p&gt;I&apos;m using the &lt;a href=&quot;https://github.com/alexei-led/todomvc-express&quot;&gt;fork&lt;/a&gt; of &lt;strong&gt;TodoMVC&lt;/strong&gt; Node.js application (by Gleb Bahmutov) as a demo application for this blog post. Feel free to clone and play with this repository.&lt;/p&gt;
&lt;p&gt;Here is the &lt;code&gt;Dockerfile&lt;/code&gt;, I&apos;ve added, for TodoMVC application. It allows to run TodoMVC application inside a Docker container.&lt;/p&gt;
&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; alpine:3.5&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# install node&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; nodejs-current tini

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/src/app
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /usr/src/app&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Build time argument to set NODE_ENV (&apos;production&apos;&apos; by default)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; NODE_ENV&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; NODE_ENV ${NODE_ENV:-production}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# install npm packages: clean obsolete files&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; package.json /usr/src/app/&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;depth 0 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm cache clean &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /tmp/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# copy source files&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . /usr/src/app&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;EXPOSE&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 3000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Set tini as entrypoint&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/sbin/tini&quot;, &quot;--&quot;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [ &quot;npm&quot;, &quot;start&quot; ]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# add VCS labels for code sync and nice reports&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; VCS_REF=&quot;local&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;LABEL&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; org.label-schema.vcs-ref=$VCS_REF \          &lt;/span&gt;
      org.label-schema.vcs-url=&quot;https://github.com/alexei-led/todomvc-express.git&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;building-and-running-todomvc-in-a-docker-container&quot;&gt;Building and Running TodoMVC in a Docker container:&lt;/h4&gt;
&lt;p&gt;To build a new Docker image for TodoMVC application, run the &lt;code&gt;docker build&lt;/code&gt; command.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# build Docker image; set VCS_REF to current HEAD commit (short)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/todomvc &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;VCS_REF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;git rev-parse &lt;span class=&quot;nt&quot;&gt;--short&lt;/span&gt; HEAD&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# run TodoMVC in a Docker container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 3000:3000 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; todomvc &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/todomvc node src/start.js
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;the-plan&quot;&gt;The Plan&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Final Goal&lt;/strong&gt; - I would like to be able to attach a Node.js debugger to a Node.js application already up and running inside a Docker container, running on remote host machine in AWS cloud, without modifying the application, container, container configuration or restarting it with additional &lt;code&gt;debug&lt;/code&gt; flags. Imagine that the application is running and there is some problem happening right now - I want to connect to it with debugger and start looking at the problem.&lt;/p&gt;
&lt;p&gt;So, I need a plan - a step by step flow that will help me to achieve the final goal.&lt;/p&gt;
&lt;p&gt;Let&apos;s start with exploring the inventory. On the server (AWS EC2 VM) machine, I have a Node.js application running inside a Docker container. On the client (my laptop), I have an IDE (Visual Studio Code, in my case), Node.js application code (&lt;code&gt;git pull/clone&lt;/code&gt;) and a Node.js debugger.&lt;/p&gt;
&lt;p&gt;So, here is my plan:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set already running application to &lt;code&gt;debug&lt;/code&gt; mode&lt;/li&gt;
&lt;li&gt;Expose a new Node.js debugger agent port to enable remote debugging in a secure way&lt;/li&gt;
&lt;li&gt;Syncronize client-server code: both should be on the same commit in &lt;code&gt;git&lt;/code&gt; tree&lt;/li&gt;
&lt;li&gt;Attach a local Node.js debugger to the Node.js debugger agent port on remote server and do it in a secure way&lt;/li&gt;
&lt;li&gt;And, if everything works, I should be able to perform regular debugging tasks, like setting breakpoints, inspecting variables, pausing execution and others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/debug_docker_node.png&quot; alt=&quot;Debug Node in Docker&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;step-1-set-already-running-nodejs-application-to-the-codedebugcode-mode&quot;&gt;Step 1: set already running Node.js application to the &lt;code&gt;debug&lt;/code&gt; mode&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The V8 debugger can be enabled and accessed either by starting Node with the &lt;code&gt;--debug&lt;/code&gt; command-line flag or by signaling an existing Node process with &lt;code&gt;SIGUSR1&lt;/code&gt;. (Node API documentation)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Cool! So, in order to switch on Node debugger agent, I just need to send the &lt;code&gt;SIGUSR1&lt;/code&gt; signal to the Node.js process of TodoMVC application. Remember, it&apos;s running inside a Docker container. What command can I use to send process signals to an application running in a Docker container?&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;docker kill&lt;/code&gt; - is my choice! This command does not actually &amp;quot;kill&amp;quot; the &lt;code&gt;PID 1&lt;/code&gt; process, running in a Docker container, but sends a &lt;a href=&quot;https://en.wikipedia.org/wiki/Unix_signal&quot;&gt;Unix signal&lt;/a&gt; to it (by default it sends &lt;code&gt;SIGKILL&lt;/code&gt;).&lt;/p&gt;
&lt;h4 id=&quot;setting-todomvc-into-codedebugcode-mode&quot;&gt;Setting TodoMVC into &lt;code&gt;debug&lt;/code&gt; mode&lt;/h4&gt;
&lt;p&gt;So, all I need to do is to send &lt;code&gt;SIGUSR1&lt;/code&gt; to my TodoMVC application running inside &lt;code&gt;todomvc&lt;/code&gt; Docker container.&lt;/p&gt;
&lt;p&gt;There are two ways to do this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use &lt;code&gt;docker kill --signal&lt;/code&gt; command to send &lt;code&gt;SIGUSR1&lt;/code&gt; to &lt;code&gt;PID 1&lt;/code&gt; process running inside Docker container, and if it&apos;s a &amp;quot;proper&amp;quot; (signal forwarding done right) init application (like &lt;code&gt;tini&lt;/code&gt;), than this will work&lt;/li&gt;
&lt;li&gt;Or execute &lt;code&gt;kill -s SIGUSR1&lt;/code&gt; inside already running Docker container, sending &lt;code&gt;SIGUSR1&lt;/code&gt; signal to the main Node.js process.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# send SIGUSR1 with docker kill (if using proper init process)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;kill&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--signal&lt;/span&gt; SIGUSR1 todomvc 
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# OR run kill command for node process inside todomvc container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; todomvc sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;kill -s SIGUSR1 $(pidof -s node)&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let&apos;s verify that Node application is set into &lt;code&gt;debug&lt;/code&gt; mode.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker logs todomvc

TodoMVC server listening at http://:::3000
emitting 2 todos
server has new 2 todos
GET / 200 31.439 ms - 3241
GET /app.css 304 4.907 ms - -
Starting debugger agent.
Debugger listening on 127.0.0.1:5858
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As you can see the Node.js debugger agent was started, but it can accept connections only from the &lt;code&gt;localhost&lt;/code&gt;, see the last output line: &lt;code&gt;Debugger listening on 127.0.0.1:5858&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;step-2-expose-node-debug-port&quot;&gt;Step 2: expose Node debug port&lt;/h3&gt;
&lt;p&gt;In order to attach a remote Node.js debugger to a Node application, running in the &lt;code&gt;debug&lt;/code&gt; mode, I need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Allow connection to debugger agent from any (or specific) IP (or IP range)&lt;/li&gt;
&lt;li&gt;Open port of Node.js debugger agent outside of Docker container&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How to do it when an application is already running in a Docker container and a Node.js debugger agent is ready to talk only with a Node.js debugger running on the same machine, plus a Node.js debugger agent port is not accessible from outside of the Docker container?&lt;/p&gt;
&lt;p&gt;Of cause it&apos;s possible to start every Node.js Docker container with exposed debugger port and allow connection from any IP (using &lt;code&gt;--debug-port&lt;/code&gt; and &lt;code&gt;--debug&lt;/code&gt; Node.js flags), but we are not looking for easy ways :).&lt;/p&gt;
&lt;p&gt;It&apos;s not a good idea from a security point of view (allowing unprotected access to a Node.js debugger). Also, if I restart an already running application with debug flags, I&apos;m going to loose the current execution context and may not be able to reproduce the problem I wanted to debug.&lt;/p&gt;
&lt;p&gt;I need a better solution!&lt;/p&gt;
&lt;p&gt;Unfortunately, Docker does not allow to expose an additional port of already running Docker container. So, I need somehow connect to a running container network and expose a new port for Node.js debugger agent.&lt;/p&gt;
&lt;p&gt;Also, it is not possible to tell a Node.js debugger agent to accept connections from different IP addresses, when Node.js process was already started.&lt;/p&gt;
&lt;p&gt;Both of above problems can be solved with help of the small Linux utility called &lt;code&gt;socat&lt;/code&gt; (SOcket CAT). This is just like the &lt;code&gt;netcat&lt;/code&gt; but with security in mind (e.g., it support chrooting) and works over various protocols and through files, pipes, devices, TCP sockets, Unix sockets, a client for SOCKS4, proxy CONNECT, or SSL etc.&lt;/p&gt;
&lt;p&gt;From &lt;code&gt;socat&lt;/code&gt; man page:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;socat&lt;/code&gt; is a command line based utility that establishes two bidirectional byte streams and transfers data between them. Because the streams can be constructed from a large set of different types of data sinks and sources (see address types), and because lots of address options may be applied to the streams, &lt;code&gt;socat&lt;/code&gt; can be used for many different purposes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Exactly, what I need!&lt;/p&gt;
&lt;p&gt;So, here is the plan. I will run a new Docker container with the &lt;code&gt;socat&lt;/code&gt; utility onboard, and configure Node.js debugger port forwarding for TodoMVC container.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;socat.Dockerfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; alpine:3.5&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; socat
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; socat -h&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;building-socat-docker-container&quot;&gt;Building socat Docker container&lt;/h4&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/socat - &amp;lt; socat.Dockerfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;allow-connection-to-node-debugger-agent-from-any-ip&quot;&gt;Allow connection to Node debugger agent from any IP&lt;/h4&gt;
&lt;p&gt;I need to run a &amp;quot;sidecar&amp;quot; &lt;code&gt;socat&lt;/code&gt; container in the same network namespace as the &lt;code&gt;todomvc&lt;/code&gt; container and define a port forwarding.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# define local port forwarding&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; socat-nid &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;container:todomvc &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/socat socat TCP-LISTEN:4848,fork TCP:127.0.0.1:5858
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now any traffic that arrives at &lt;code&gt;4848&lt;/code&gt; port will be routed to the Node.js debugger agent listening on &lt;code&gt;127.0.0.1:5858&lt;/code&gt;. The &lt;code&gt;4848&lt;/code&gt; port can accept traffic from any IP.
It&apos;s possible to use an IP range to restrict connection to the &lt;code&gt;socat&lt;/code&gt; listening port, adding &lt;code&gt;range=&amp;lt;ANY IP RANGE&amp;gt;&lt;/code&gt; option.&lt;/p&gt;
&lt;h4 id=&quot;exposing-nodejs-debugger-port-from-docker-container&quot;&gt;Exposing Node.js debugger port from Docker container&lt;/h4&gt;
&lt;p&gt;First, we will get IP of &lt;code&gt;todomvc&lt;/code&gt; Docker container.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# get IP of todomvc container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ TODOMVC_IP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker inspect &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;\{\{.NetworkSettings.IPAddress\}\}&apos;&lt;/span&gt; todomvc&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then, configure port forwarding to the &amp;quot;sidecar&amp;quot; &lt;code&gt;socat&lt;/code&gt; port, we define previously, running on the same network as the &lt;code&gt;todomvc&lt;/code&gt; container.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# run socat container to expose Node.js debugger agent port forwarder&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 5858:5858 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; socat &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/socat socat TCP-LISTEN:5858,fork TCP:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TODOMVC_IP&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:4848
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Any traffic that will arrive at the &lt;code&gt;5858&lt;/code&gt; port on the Docker host will be forwarded, first, to the &lt;code&gt;4848&lt;/code&gt; socat port and then to the Node.js debugger agent running inside the &lt;code&gt;todomvc&lt;/code&gt; Docker container.&lt;/p&gt;
&lt;h4 id=&quot;exposing-nodejs-debugger-port-for-remote-access&quot;&gt;Exposing Node.js debugger port for remote access&lt;/h4&gt;
&lt;p&gt;In most cases, I would like to debug an application running on a remote machine (AWS EC2 instance, for example). I also do not want to expose a Node.js debugger agent port unprotected to the whole world.&lt;/p&gt;
&lt;p&gt;One possible and working solution is to use SSH tunneling to access this port.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Open SSH Tunnel to gain access to servers port 5858. Set `SSH_KEY_FILE` to ssh key location or add it to ssh-agent&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# open an ssh tunnel, send it to the bg, and wait 20 seconds for connections&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# once all connections are closed after 20 seconds then close the tunnel&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;SSH_KEY_FILE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ExitOnForwardFailure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; 5858:127.0.0.1:5858 ec2_user@some.ec2.host.com &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;20
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now all traffic to the &lt;code&gt;localhost:5858&lt;/code&gt; will be tunneled over &lt;code&gt;SSH&lt;/code&gt; to the remote Docker host machine and after some &lt;code&gt;socat&lt;/code&gt; forwarding to the Node.js debugger agent running inside the &lt;code&gt;todomvc&lt;/code&gt; container.&lt;/p&gt;
&lt;h3 id=&quot;step-3-synchronizing-on-the-same-code-commit&quot;&gt;Step 3: Synchronizing on the same code commit&lt;/h3&gt;
&lt;p&gt;In order to be able to debug a remote application, you need to make sure that you are using the same code in your IDE as one that is running on remote server.&lt;/p&gt;
&lt;p&gt;I will try to automate this step too. Remember the &lt;code&gt;LABEL&lt;/code&gt; command, I&apos;ve used in TodoMVC &lt;code&gt;Dockerfile&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;These labels help me to identify git repository and commit for the application Docker image:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;org.label-schema.vcs-ref&lt;/code&gt; - contains short SHA for a &lt;code&gt;HEAD&lt;/code&gt; commit&lt;/li&gt;
&lt;li&gt;&lt;code&gt;org.label-schema.vcs-url&lt;/code&gt; - contains an application git repository url (I can use in &lt;code&gt;clone/pull&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&apos;m using (Label Schema Convention)[http://label-schema.org/rc1/], since I really like it and find it useful, but you can select any other convention too.&lt;/p&gt;
&lt;p&gt;This approach allows me, for each, properly labeled, Docker image, to identify the application code repository and the commit it was created from.&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# get git repository url form Docker image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ GIT_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker inspect &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/todomvc | jq &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;.[].ContainerConfig.Labels.&quot;org.label-schema.vcs-url&quot;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# get git commit from Docker image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ GIT_COMMIT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker inspect &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;/todomvc | jq &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;.[].ContainerConfig.Labels.&quot;org.label-schema.vcs-ref&quot;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# clone git repository, if needed&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone &lt;span class=&quot;nv&quot;&gt;$GIT_URL&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# set HEAD to same commit as server&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git checkout &lt;span class=&quot;nv&quot;&gt;$GIT_COMMIT&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now, both my local development environment and remote application are on the same git commit. And I can start to debug my code, finally!&lt;/p&gt;
&lt;h3 id=&quot;step-4-attaching-local-nodejs-debugger-to-debugger-agent-port&quot;&gt;Step 4: Attaching local Node.js debugger to debugger agent port&lt;/h3&gt;
&lt;p&gt;To start debugging, I need to configure my IDE. In my case, it&apos;s &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;Visual Studio Code&lt;/a&gt; and I need to add a new &lt;code&gt;Launch&lt;/code&gt; configuration.&lt;/p&gt;
&lt;p&gt;This launch configuration specifies remote debugger server and port to attach and remote location for application source files, which should be in sync with local files (see the previous step).&lt;/p&gt;
&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;For&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;more&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;information&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;about&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Node.js&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;attributes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;visit:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;https://go.microsoft.com/fwlink/?linkid=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;830387&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.2.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;configurations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;node&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;request&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;attach&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Debug Remote Docker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;port&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5858&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;localRoot&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;${workspaceRoot}/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;remoteRoot&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/usr/src/app/&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;And finally, I&apos;ve met my goal: I&apos;m able to attach a Node.js debugger to a Node.js application, that is already up and running in a Docker container on a remote machine.&lt;/p&gt;
&lt;p&gt;It was a long journey to find the proper solution, but after I found it, the process does not look complex at all. Now, once I met a new problem in our environment I can easily attach the Node.js debugger to the running application and start exploring the problem. Nice, isn&apos;t it?&lt;/p&gt;
&lt;p&gt;I&apos;ve recorded a short movie, just to demonstrate all steps and prove that things are working fluently, exactly as I&apos;ve described in this post.&lt;/p&gt;
&lt;div class=&quot;embed-container&quot;&gt;
  &amp;lt;iframe
      src=&quot;https://www.youtube.com/embed/WYOfNTJmE_4&quot;
      width=&quot;700&quot;
      height=&quot;480&quot;
      frameborder=&quot;0&quot;
      allowfullscreen=&quot;&quot;&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;Hope, you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Docker" /><category term="docker" /><category term="tutorial" /><category term="devops" /><category term="hacks" /><category term="node" /><category term="node.js" /><category term="debug" /><category term="Dockerfile" /><summary type="html">Teaser</summary></entry><entry><title type="html">Create lean Node.js image with Docker multi-stage build</title><link href="http://localhost:4000/docker/2017/04/25/node-docker-multistage.html" rel="alternate" type="text/html" title="Create lean Node.js image with Docker multi-stage build" /><published>2017-04-25T19:00:00+03:00</published><updated>2017-04-25T19:00:00+03:00</updated><id>http://localhost:4000/docker/2017/04/25/node-docker-multistage</id><content type="html" xml:base="http://localhost:4000/docker/2017/04/25/node-docker-multistage.html">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Starting from Docker 17.05+, you can create a single &lt;code&gt;Dockerfile&lt;/code&gt; that can build multiple helper images with compilers, tools, and tests and use files from above images to produce the &lt;strong&gt;final&lt;/strong&gt; Docker image.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/multi_stage_build.png&quot; alt=&quot;Multi-stage Docker Build&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-quotcore-principlequot-of-dockerfile&quot;&gt;The &amp;quot;core principle&amp;quot; of Dockerfile&lt;/h2&gt;
&lt;p&gt;Docker can build images by reading the instructions from a &lt;code&gt;Dockerfile&lt;/code&gt;. A &lt;code&gt;Dockerfile&lt;/code&gt; is a text file that contains a list of all the commands needed to build a new Docker image. The syntax of &lt;code&gt;Dockerfile&lt;/code&gt; is pretty simple and the Docker team tries to keep it intact between Docker engine releases.&lt;/p&gt;
&lt;p&gt;The core principle is very simple: &lt;code&gt;1 Dockerfile -&amp;gt; 1 Docker Image&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This principle works just fine for basic use cases, where you just need to demonstrate Docker capabilities or put some &amp;quot;static&amp;quot; content into a Docker image.&lt;/p&gt;
&lt;p&gt;Once you advance with Docker and would like to create secure and lean Docker images, singe &lt;code&gt;Dockerfile&lt;/code&gt; is not enough.&lt;/p&gt;
&lt;p&gt;People who insist on following the above principle find themselves with slow Docker builds, huge Docker images (several GB size images), slow deployment time and lots of CVE violations embedded into these images.&lt;/p&gt;
&lt;h2 id=&quot;the-docker-build-container-pattern&quot;&gt;The Docker Build Container pattern&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/@alexeiled/docker-pattern-the-build-container-b0d0e86ad601&quot;&gt;Docker Pattern: The Build Container&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The basic idea behind &lt;strong&gt;Build Container&lt;/strong&gt; pattern is simple:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Create additional Docker images with required tools (compilers, linters, testing tools) and use these images to produce lean, secure and production ready Docker image.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;=============&lt;/p&gt;
&lt;p&gt;An example of the &lt;strong&gt;Build Container&lt;/strong&gt; pattern for typical Node.js application:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Derive &lt;code&gt;FROM&lt;/code&gt; a Node base image (for example &lt;code&gt;node:6.10-alpine&lt;/code&gt;) &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;npm&lt;/code&gt; installed (&lt;code&gt;Dockerfile.build&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Add &lt;code&gt;package.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Install all node modules from &lt;code&gt;dependency&lt;/code&gt; and &lt;code&gt;devDependency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy application code&lt;/li&gt;
&lt;li&gt;Run compilers, code coverage, linters, code analysis and testing tools&lt;/li&gt;
&lt;li&gt;Create the &lt;strong&gt;production&lt;/strong&gt; Docker image; derive &lt;code&gt;FROM&lt;/code&gt; same or other Node base image&lt;/li&gt;
&lt;li&gt;install node modules required for runtime (&lt;code&gt;npm install --only=production&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;expose &lt;code&gt;PORT&lt;/code&gt; and define default &lt;code&gt;CMD&lt;/code&gt; (command to run your application)&lt;/li&gt;
&lt;li&gt;Push the &lt;strong&gt;production&lt;/strong&gt; image to some Docker registry&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This flow assumes that you are using two or more separate &lt;code&gt;Dockerfile&lt;/code&gt;s and a shell script or flow tool to orchestrate all steps above.&lt;/p&gt;
&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;
&lt;p&gt;I use a fork of &lt;a href=&quot;https://github.com/sdelements/lets-chat&quot;&gt;Let&apos;s Chat&lt;/a&gt; node.js application.&lt;/p&gt;
&lt;h4 id=&quot;builder-docker-image-with-eslint-mocha-and-gulp&quot;&gt;Builder Docker image with eslint, mocha and gulp&lt;/h4&gt;
&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; alpine:3.5&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; nodejs
&lt;span class=&quot;c&quot;&gt;# set working directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /root/chat&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy project file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; package.json .&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node packages&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;depth 0 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy app files&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . .&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# run linter, setup and tests&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; npm run lint &amp;amp;&amp;amp; npm run setup &amp;amp;&amp;amp; npm run test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;production-docker-image-with-production-node-modules-only&quot;&gt;Production Docker image with &apos;production&apos; node modules only&lt;/h4&gt;
&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; alpine:3.5&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; nodejs tini
&lt;span class=&quot;c&quot;&gt;# set working directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /root/chat&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy project file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; package.json .&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node packages&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;depth 0 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--only&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;production &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;    npm cache clean
&lt;span class=&quot;c&quot;&gt;# copy app files&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . .&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Set tini as entrypoint&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/sbin/tini&quot;, &quot;--&quot;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# application server port&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;EXPOSE&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 5000&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# default run command&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; npm run start&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;what-is-docker-multi-stage-build&quot;&gt;What is Docker multi-stage build?&lt;/h2&gt;
&lt;p&gt;Docker 17.0.5 extends &lt;code&gt;Dockerfile&lt;/code&gt; syntax to support new &lt;strong&gt;multi-stage&lt;/strong&gt; build, by extending two commands: &lt;code&gt;FROM&lt;/code&gt; and &lt;code&gt;COPY&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;multi-stage&lt;/strong&gt; build allows using multiple &lt;code&gt;FROM&lt;/code&gt; commands in the same Dockerfile. The last &lt;code&gt;FROM&lt;/code&gt; command produces the final Docker image, all other images are intermediate images (no final Docker image is produced, but &lt;em&gt;all layers are cached&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;FROM&lt;/code&gt; syntax also supports &lt;code&gt;AS&lt;/code&gt; keyword. Use &lt;code&gt;AS&lt;/code&gt; keyword to give the current image a logical name and reference to it later by this name.&lt;/p&gt;
&lt;p&gt;To copy files from intermediate images use &lt;code&gt;COPY --from=&amp;lt;image_AS_name|image_number&amp;gt;&lt;/code&gt;, where number starts from &lt;code&gt;0&lt;/code&gt; (but better to use logical name through &lt;code&gt;AS&lt;/code&gt; keyword).&lt;/p&gt;
&lt;h2 id=&quot;creating-a-multi-stage-dockerfile-for-nodejs-application&quot;&gt;Creating a multi-stage Dockerfile for Node.js application&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; below makes the &lt;strong&gt;Build Container&lt;/strong&gt; pattern obsolete, allowing to achieve the same result with the single file.&lt;/p&gt;
&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---- Base Node ----&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; alpine:3.5 AS base&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; nodejs-npm tini
&lt;span class=&quot;c&quot;&gt;# set working directory&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /root/chat&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Set tini as entrypoint&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/sbin/tini&quot;, &quot;--&quot;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy project file&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; package.json .&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---- Dependencies ----&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; base AS dependencies&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# install node packages&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; npm config &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;depth 0
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--only&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;production 
&lt;span class=&quot;c&quot;&gt;# copy production node_modules aside&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; node_modules prod_node_modules
&lt;span class=&quot;c&quot;&gt;# install ALL node_modules, including &apos;devDependencies&apos;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---- Test ----&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# run linters, setup and tests&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; dependencies AS test&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . .&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN  &lt;/span&gt;npm run lint &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; npm run setup &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; npm run &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---- Release ----&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; base AS release&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy production node_modules&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; --from=dependencies /root/chat/prod_node_modules ./node_modules&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy app sources&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . .&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# expose port and define CMD&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;EXPOSE&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; 5000&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; npm run start&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The above &lt;code&gt;Dockerfile&lt;/code&gt; creates 3 intermediate Docker images and single &lt;strong&gt;release&lt;/strong&gt; Docker image (the final &lt;code&gt;FROM&lt;/code&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First image &lt;code&gt;FROM alpine:3.5 AS bas&lt;/code&gt; - is a base Node image with: &lt;code&gt;node&lt;/code&gt;, &lt;code&gt;npm&lt;/code&gt;, &lt;code&gt;tini&lt;/code&gt; (init app) and &lt;code&gt;package.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Second image &lt;code&gt;FROM base AS dependencies&lt;/code&gt; - contains all node modules from &lt;code&gt;dependencies&lt;/code&gt; and &lt;code&gt;devDependencies&lt;/code&gt; with additional copy of &lt;code&gt;dependencies&lt;/code&gt; required for final image only&lt;/li&gt;
&lt;li&gt;Third image &lt;code&gt;FROM dependencies AS test&lt;/code&gt; - runs linters, setup and tests (with &lt;code&gt;mocha&lt;/code&gt;); if this run command fail not final image is produced&lt;/li&gt;
&lt;li&gt;The final image &lt;code&gt;FROM base AS release&lt;/code&gt; - is a base Node image with application code and all node modules from &lt;code&gt;dependencies&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;try-docker-multi-stage-build-today&quot;&gt;Try Docker multi-stage build today&lt;/h2&gt;
&lt;p&gt;In order to try Docker &lt;strong&gt;multi-stage&lt;/strong&gt; build, you need to get Docker 17.0.5, which is going to be released in May and currently available on the &lt;em&gt;beta&lt;/em&gt; channel.&lt;/p&gt;
&lt;p&gt;So, you have two options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use &lt;em&gt;beta&lt;/em&gt; channel to get Docker 17.0.5&lt;/li&gt;
&lt;li&gt;Run &lt;em&gt;dind&lt;/em&gt; container (docker-in-docker)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;running-docker-in-docker-1705-beta&quot;&gt;Running Docker-in-Docker 17.0.5 (beta)&lt;/h3&gt;
&lt;p&gt;Running Docker 17.0.5 (beta) in docker container (&lt;code&gt;--privileged&lt;/code&gt; is required):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -d --rm --privileged -p 23751:2375 --name dind docker:17.05.0-ce-dind --storage-driver overlay2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try &lt;strong&gt;mult-stage&lt;/strong&gt; build. Add &lt;code&gt;--host=:23751&lt;/code&gt; to every Docker command, or set &lt;code&gt;DOCKER_HOST&lt;/code&gt; environment variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ # using --host
$ docker --host=:23751 build -t local/chat:multi-stage .

$ # OR: setting DOCKER_HOST
$ export DOCKER_HOST=localhost:23751
$ docker build -t local/chat:multi-stage .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;With Docker &lt;strong&gt;multi-stage&lt;/strong&gt; build feature, it&apos;s possible to implement an advanced Docker image build pipeline using a single &lt;code&gt;Dockerfile&lt;/code&gt;. Kudos to Docker team!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Hope, you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Docker" /><category term="docker" /><category term="tutorial" /><category term="devops" /><category term="hacks" /><category term="node" /><category term="node.js" /><category term="multistage" /><category term="Dockerfile" /><summary type="html">TL;DR</summary></entry><entry><title type="html">Crafting perfect Java Docker build flow</title><link href="http://localhost:4000/devops/2017/03/07/perfect-docker-java.html" rel="alternate" type="text/html" title="Crafting perfect Java Docker build flow" /><published>2017-03-07T18:00:00+02:00</published><updated>2017-03-07T18:00:00+02:00</updated><id>http://localhost:4000/devops/2017/03/07/perfect-docker-java</id><content type="html" xml:base="http://localhost:4000/devops/2017/03/07/perfect-docker-java.html">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;What is the bare minimum you need to &lt;strong&gt;build&lt;/strong&gt;, &lt;strong&gt;test&lt;/strong&gt; and &lt;strong&gt;run&lt;/strong&gt; my Java application in Docker container?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The recipe:&lt;/strong&gt; Create a separate Docker image for each step and optimize the way you are running it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/duke_docker.png&quot; alt=&quot;Duke and Container&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I started working with Java in 1998, and for a long time, it was my main programming language. It was a long love‚Äìhate relationship.&lt;/p&gt;
&lt;p&gt;DDuring my work career, I wrote a lot of code in Java. Despite that fact, I don‚Äôt think Java is usually the right choice for writing microservices running in &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; containers.&lt;/p&gt;
&lt;p&gt;But, sometimes you have to work with Java. Maybe Java is your favorite language and you do not want to learn a new one, or you have a legacy code that you need to maintain, or your company decided on Java and you have no other option.&lt;/p&gt;
&lt;p&gt;Whatever reason you have to &lt;strong&gt;&lt;em&gt;marry Java with Docker&lt;/em&gt;&lt;/strong&gt;, you better &lt;strong&gt;&lt;em&gt;do it properly&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I will show you how to create an effective Java-Docker build pipeline to consistently produce small, efficient, and secure Docker images.&lt;/p&gt;
&lt;h2 id=&quot;be-careful&quot;&gt;Be careful&lt;/h2&gt;
&lt;p&gt;There are plenty of &lt;em&gt;‚ÄúDocker for Java developers‚Äù&lt;/em&gt; tutorials out there, that unintentionally encourage some Docker bad practices.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://sparktutorials.github.io/2015/04/14/getting-started-with-spark-and-docker.html&quot;&gt;Spark and Docker tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://examples.javacodegeeks.com/devops/docker/introduction-docker-java-developers/&quot;&gt;Introducing Docker for Java Developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.developer.com/java/data/using-java-with-docker-engine.html&quot;&gt;Using Java with Docker Engine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and others ...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For current demo project, first two tutorials took around 15 minutes to build (first build) and produced images of 1.3GB size each.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make yourself a favor and do not follow these tutorials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;what-should-you-know-about-docker&quot;&gt;What should you know about Docker?&lt;/h2&gt;
&lt;p&gt;Developers new to Docker are often tempted to think of it as just another VM. Instead, think of Docker as a ‚Äúchild process‚Äù. The files and packages needed for an entire VM are different from those needed by just another process running a dev machine. Docker is even better than a child process because it allows better isolation and environmental control.&lt;/p&gt;
&lt;p&gt;If you‚Äôre new to Docker, I suggest reading this &lt;a href=&quot;https://docs.docker.com/engine/understanding-docker/&quot;&gt;Understanding Docker&lt;/a&gt; article. Docker isn‚Äôt so complex than any developer should not be able to understand how it works.&lt;/p&gt;
&lt;h2 id=&quot;dockerizing-java-application&quot;&gt;Dockerizing Java application&lt;/h2&gt;
&lt;h3 id=&quot;what-files-need-to-be-included-in-a-java-applications-docker-image&quot;&gt;What files need to be included in a Java Application‚Äôs Docker image?&lt;/h3&gt;
&lt;p&gt;Since Docker containers are just isolated processes, your Java Docker image should only contain the files required to run your application.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What are these files?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It starts with a Java Runtime Environment (&lt;strong&gt;JRE&lt;/strong&gt;). &lt;strong&gt;JRE&lt;/strong&gt; is a software package, that has everything required to run a Java program. It includes an implementation of the Java Virtual Machine (&lt;strong&gt;JVM&lt;/strong&gt;) with an implementation of the &lt;em&gt;Java Class Library&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I recommend using &lt;a href=&quot;http://openjdk.java.net/&quot;&gt;OpenJDK&lt;/a&gt; JRE. OpenJDK is licensed under &lt;a href=&quot;https://en.wikipedia.org/wiki/GNU_General_Public_License&quot;&gt;GPL&lt;/a&gt; with &lt;a href=&quot;http://www.gnu.org/software/classpath/license.html&quot;&gt;Classpath Exception&lt;/a&gt;. The &lt;em&gt;Classpath Exception&lt;/em&gt; part is important. This license allows using OpenJDK with any software of any license, not just the GPL. In particular, you can use OpenJDK in proprietary software without disclosing your code.&lt;/p&gt;
&lt;p&gt;Before using Oracle‚Äôs JDK/JRE, please read the following post: &lt;a href=&quot;http://blog.takipi.com/running-java-on-docker-youre-breaking-the-law/&quot;&gt;‚ÄúRunning Java on Docker? You‚Äôre Breaking the Law.‚Äù&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since it‚Äôs rare for Java applications to be developed using only the standard library, you most likely need to also add 3rd party Java libraries. Then add the application compiled bytecode as plain &lt;em&gt;Java Class&lt;/em&gt; files or packaged into &lt;em&gt;JAR&lt;/em&gt; archives. And, if you are using native code, you will need to add corresponding native libraries/packages too.&lt;/p&gt;
&lt;h3 id=&quot;choosing-a-base-docker-image-for-java-application&quot;&gt;Choosing a base Docker image for Java Application&lt;/h3&gt;
&lt;p&gt;In order to choose the base Docker image, you need to answer the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;What native packages do you need for your Java application?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Should you choose Ubuntu or Debian as your base image?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;What is your strategy for patching security holes, including packages you are not using at all?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Do you mind paying extra (money and time) for network traffic and storage of unused files?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some might say: &lt;em&gt;‚Äúbut, if all your images share the same Docker layers, you only download them just once, right?‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;That‚Äôs &lt;em&gt;true&lt;/em&gt; in theory, but in reality is often very different.&lt;/p&gt;
&lt;p&gt;Usually, you have lots of different images: some you built lately, others a long time ago, others you pull from DockerHub. All these images do not share the same base image or version. You need to invest a lot of time to align these images to share the same base image and then keep these images up-to-date.&lt;/p&gt;
&lt;p&gt;Some might say: &lt;em&gt;‚Äúbut, who cares about image size? we download them just once and run forever‚Äù&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Docker image size is actually very important.&lt;/p&gt;
&lt;p&gt;The size has an impact on ‚Ä¶&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;network latency&lt;/strong&gt; - need to transfer Docker image over the web&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;storage&lt;/strong&gt; - need to store all these bits somewhere&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;service availability and elasticity&lt;/strong&gt; - when using a Docker scheduler, like Kubernetes, Swarm, DC/OS or other (scheduler can move containers between hosts)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;security&lt;/strong&gt; - do you really, I mean really need the libpng package with all its &lt;a href=&quot;https://www.cvedetails.com/vulnerability-list/vendor_id-7294/Libpng.html&quot;&gt;CVE vulnerabilities&lt;/a&gt; for your Java application?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;development agility&lt;/strong&gt; - small Docker images == faster build time and faster deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without being careful, Java Docker images tends to grow to enormous sizes. I‚Äôve seen 3GB Java images, where the real code and required JAR libraries only take around 150MB.&lt;/p&gt;
&lt;p&gt;Consider using &lt;a href=&quot;https://hub.docker.com/_/alpine/&quot;&gt;Alpine Linux image&lt;/a&gt;, which is only a 5MBs image, as a base Docker image. Lots of &lt;a href=&quot;https://github.com/docker-library/official-images&quot;&gt;&amp;quot;Official Docker images&amp;quot;&lt;/a&gt; have an Alpine-based flavor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Many, but not all Linux packages have versions compiled with &lt;code&gt;musl libc&lt;/code&gt; C runtime library. Sometimes you want to use a package that is compiled with &lt;code&gt;glibc&lt;/code&gt; (GNU C runtime library). The &lt;a href=&quot;https://hub.docker.com/r/frolvlad/alpine-glibc/&quot;&gt;frolvlad/alpine-glibc&lt;/a&gt; image based on Alpine Linux image and contains &lt;code&gt;glibc&lt;/code&gt; to enable proprietary projects, compiled against &lt;code&gt;glibc&lt;/code&gt; (e.g. OracleJDK, Anaconda), working on Alpine.&lt;/p&gt;
&lt;h3 id=&quot;choosing-the-right-java-application-server&quot;&gt;Choosing the right Java Application server&lt;/h3&gt;
&lt;p&gt;Frequently, you also need to expose some kind of interface to reach your Java application, that runs in a Docker container.&lt;/p&gt;
&lt;p&gt;When you deploy Java applications with Docker containers, the default Java deployment model changes.&lt;/p&gt;
&lt;p&gt;Originally, Java server-side deployment assumes that you have already pre-configured a Java Web Server (Tomcat, WebLogic, JBoss, or other) and you are deploying an application &lt;strong&gt;WAR&lt;/strong&gt; (Web Archive) packaged Java application to this server and run it together with other applications, deployed on the same server.&lt;/p&gt;
&lt;p&gt;Lots of tools are developed around this concept, allowing you to update running applications without stopping the Java Application server, route traffic to the new application, resolve possible class loading conflicts and more.&lt;/p&gt;
&lt;p&gt;With Docker-based deployments, you do not need these tools anymore, you don&apos;t even need the fat &amp;quot;enterprise-ready&amp;quot; Java Application servers. The only thing that you need is a stable and scalable network server that can serve your API over HTTP/TCP or other protocol of your choice. Search Google for [‚Äúembedded Java server‚Äù](&lt;a href=&quot;https://www.google.com/search?q=%22embedded&quot;&gt;https://www.google.com/search?q=&amp;quot;embedded&lt;/a&gt; java server&amp;quot;) and take one that you like most.&lt;/p&gt;
&lt;p&gt;For this demo, I forked &lt;a href=&quot;https://github.com/khoubyari/spring-boot-rest-example&quot;&gt;Spring Boot&apos;s REST example&lt;/a&gt; and modified it a bit. The demo uses &lt;a href=&quot;https://projects.spring.io/spring-boot/&quot;&gt;Spring Boot&lt;/a&gt; with an embedded &lt;a href=&quot;http://tomcat.apache.org/&quot;&gt;Tomcat&lt;/a&gt; server. Here is my &lt;a href=&quot;https://github.com/alexei-led/spring-boot-rest-example&quot;&gt;fork&lt;/a&gt; on GitHub repository (&lt;code&gt;blog&lt;/code&gt; branch).&lt;/p&gt;
&lt;h3 id=&quot;building-a-java-application-docker-image&quot;&gt;Building a Java Application Docker image&lt;/h3&gt;
&lt;p&gt;In order to run this demo, I need to create a Docker image with JRE, the compiled and packaged Java application, and all 3rd party libraries.&lt;/p&gt;
&lt;p&gt;Here is the &lt;code&gt;Dockerfile&lt;/code&gt; I used to build my Docker image. This demo Docker image is based on slim Alpine Linux with OpenJDK JRE and contains the application WAR file with all dependencies embedded into it. It&apos;s just the bare minimum required to run the demo application.&lt;/p&gt;
&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Base Alpine Linux based image with OpenJDK JRE only&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; openjdk:8-jre-alpine&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# copy application WAR (with libraries inside)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; target/spring-boot-*.war /app.war&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# specify default command&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/usr/bin/java&quot;, &quot;-jar&quot;, &quot;-Dspring.profiles.active=test&quot;, &quot;/app.war&quot;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To build the Docker image, run the following command:&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; blog/sbdemo:latest &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Running the &lt;code&gt;docker history&lt;/code&gt; command on created Docker image will let you to see all layers that make up this image:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4.8MB Alpine Linux Layer&lt;/li&gt;
&lt;li&gt;103MB OpenJDK JRE Layer&lt;/li&gt;
&lt;li&gt;61.8MB Application WAR file&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;$ docker history blog/sbdemo:latest

IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
16d5236aa7c8        About an hour ago   /bin/sh -c #(nop)  CMD [&amp;quot;/usr/bin/java&amp;quot; &amp;quot;-...   0 B                 
e1bbd125efc4        About an hour ago   /bin/sh -c #(nop) COPY file:1af38329f6f390...   61.8 MB             
d85b17c6762e        2 months ago        /bin/sh -c set -x  &amp;amp;&amp;amp; apk add --no-cache  ...   103 MB              
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop)  ENV JAVA_ALPINE_VERSION...   0 B                 
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop)  ENV JAVA_VERSION=8u111       0 B                 
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop)  ENV PATH=/usr/local/sbi...   0 B                 
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop)  ENV JAVA_HOME=/usr/lib/...   0 B                 
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c {   echo &apos;#!/bin/sh&apos;;   echo &apos;s...   87 B                
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0 B                 
&amp;lt;missing&amp;gt;           2 months ago        /bin/sh -c #(nop) ADD file:eeed5f514a35d18...   4.8 MB              
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;running-the-java-application-docker-container&quot;&gt;Running the Java Application Docker container&lt;/h3&gt;
&lt;p&gt;In order to run the demo application, run following command:&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; demo-default &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8090:8090 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8091:8091 blog/sbdemo:latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let&apos;s check, that application is up and running (I‚Äôm using the &lt;code&gt;httpie&lt;/code&gt; tool here):&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;http http://localhost:8091/info

HTTP/1.1 200 OK
Content-Type: application/json
Date: Thu, 09 Mar 2017 14:43:28 GMT
Server: Apache-Coyote/1.1
Transfer-Encoding: chunked

&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;build&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;artifact&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.artifactId&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;description&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;boot-example default description&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;spring-boot-rest-example&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;version&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;0.1&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;setting-docker-container-memory-constraints&quot;&gt;Setting Docker container memory constraints&lt;/h4&gt;
&lt;p&gt;One thing you need to know about Java process memory allocation is that in reality it consumes more physical memory than specified with the &lt;code&gt;-Xmx&lt;/code&gt; JVM option. The &lt;code&gt;-Xmx&lt;/code&gt; option specifies only the maximum Java heap size. But the Java process is a regular Linux process and what is interesting, is how much actual physical memory this process is consuming.&lt;/p&gt;
&lt;p&gt;Or in other words - &lt;em&gt;what is the &lt;strong&gt;Resident Set Size&lt;/strong&gt; (&lt;strong&gt;RSS&lt;/strong&gt;) value for running a Java process?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Theoretically, in the case of a Java application, a required RSS size can be calculated by:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RSS = Heap size + MetaSpace + OffHeap size
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;em&gt;OffHeap&lt;/em&gt; consists of thread stacks, direct buffers, mapped files (libraries and jars) and JVM code itself.&lt;/p&gt;
&lt;p&gt;There is a very good post on this topic: &lt;a href=&quot;http://trustmeiamadeveloper.com/2016/03/18/where-is-my-memory-java/&quot;&gt;Analyzing java memory usage in a Docker container&lt;/a&gt; by Mikhail Krestjaninoff.&lt;/p&gt;
&lt;p&gt;When using the  &lt;code&gt;--memory&lt;/code&gt;  option in &lt;code&gt;docker run&lt;/code&gt; make sure the limit is larger (at least twice) than what you specify for &lt;code&gt;-Xmx&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&quot;offtopic-using-oom-killer-instead-of-gc&quot;&gt;Offtopic: Using OOM Killer instead of GC&lt;/h4&gt;
&lt;p&gt;There is an interesting &lt;strong&gt;JDK Enhancement Proposal (JEP)&lt;/strong&gt; by Aleksey Shipilev: [Epsilon GC]((&lt;a href=&quot;http://openjdk.java.net/jeps/8174901&quot;&gt;http://openjdk.java.net/jeps/8174901&lt;/a&gt;). This JEP proposes to develop a GC that only handles memory allocation, but does not implement any actual memory reclamation mechanism.&lt;/p&gt;
&lt;p&gt;This GC, combined with &lt;code&gt;--restart&lt;/code&gt; (Docker restart policy) should theoretically allow supporting ‚ÄúExtremely short lived jobs‚Äù implemented in Java.&lt;/p&gt;
&lt;p&gt;For ultra-performance-sensitive applications, where developers are conscious about memory allocations or want to create completely garbage-free applications - GC cycle may be considered an implementation bug that wastes cycles for no good reason. In such use case, it could be better to allow &lt;strong&gt;OOM Killer&lt;/strong&gt; (Out of Memory) to kill the process and use Docker restart policy to restarting the process.&lt;/p&gt;
&lt;p&gt;Anyway, &lt;strong&gt;Epsilon GC&lt;/strong&gt; is not available yet, so it‚Äôs just an interesting theoretical use case for a moment.&lt;/p&gt;
&lt;h2 id=&quot;building-java-applications-with-builder-container&quot;&gt;Building Java applications with Builder container&lt;/h2&gt;
&lt;p&gt;As you can probably see, in the previous step, I did not explain how I‚Äôve created the application WAR file.&lt;/p&gt;
&lt;p&gt;Of course, there is a Maven project file &lt;code&gt;pom.xml&lt;/code&gt; which most Java developers should be familiar with. But, in order to actually build, you need to install the &lt;em&gt;same Java Build tools&lt;/em&gt; (JDK and Maven) on &lt;em&gt;every machine&lt;/em&gt;, where you are building the application. You need to have the same versions, use the same repositories and share the same configurations. While&apos;s tt‚Äôs possible, managing different projects that rely on different tools, versions, configurations, and development environments can quickly become a nightmare.&lt;/p&gt;
&lt;p&gt;What if you might also want to run a build on a clean machine that does not have Java or Maven installed? &lt;em&gt;What should you do?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;java-builder-container&quot;&gt;Java Builder Container&lt;/h3&gt;
&lt;p&gt;Docker can help here too. With Docker, you can create and share portable development and build environments. The idea is to create a special &lt;strong&gt;Builder&lt;/strong&gt; Docker image, that contains all tools you need to properly build your Java application, e.g.: JDK, Ant, Maven, Gradle, SBT or others.&lt;/p&gt;
&lt;p&gt;To create a really useful &lt;strong&gt;Builder&lt;/strong&gt; Docker image, you need to know well how you Java Build tools are working and how &lt;code&gt;docker build&lt;/code&gt; invalidates build cache. Without proper design, you will end up with non-effective and slow builds.&lt;/p&gt;
&lt;h3 id=&quot;running-maven-in-docker&quot;&gt;Running Maven in Docker&lt;/h3&gt;
&lt;p&gt;While most of these tools were created nearly a generation ago, they are still are very popular and widely used by Java developers.&lt;/p&gt;
&lt;p&gt;Java development life is hard to imagine without some extra build tools. There are multiple Java build tools out there, but most of them share similar concepts and serve the same targets - resolve cumbersome package dependencies, and run different build tasks, such as, &lt;strong&gt;compile, lint, test, package, and deploy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I will use &lt;a href=&quot;https://maven.apache.org&quot;&gt;Maven&lt;/a&gt;, but the same approach can be applied to &lt;a href=&quot;https://gradle.org/&quot;&gt;Gradle&lt;/a&gt;, &lt;a href=&quot;http://www.scala-sbt.org/&quot;&gt;SBT&lt;/a&gt;, and other less popular Java Build tools.&lt;/p&gt;
&lt;p&gt;It‚Äôs important to learn how your Java Build tool works and how can it&apos;s tuned. Apply this knowledge, when creating a &lt;strong&gt;Builder&lt;/strong&gt; Docker image and the way you are running a &lt;strong&gt;Builder&lt;/strong&gt; Docker container.&lt;/p&gt;
&lt;p&gt;Maven uses the project level &lt;code&gt;pom.xml&lt;/code&gt; file to resolve project dependencies. It downloads missing &lt;code&gt;JAR&lt;/code&gt; files from private and public Maven repositories, and &lt;em&gt;caches&lt;/em&gt; these files for future builds. Thus, next time you run your build, it won‚Äôt download anything if your dependency had not been changed.&lt;/p&gt;
&lt;h4 id=&quot;official-maven-docker-image-should-you-use-it&quot;&gt;Official Maven Docker image: should you use it?&lt;/h4&gt;
&lt;p&gt;The Maven team provides an official &lt;a href=&quot;https://hub.docker.com/r/_/maven/&quot;&gt;Docker images&lt;/a&gt;. There are multiple images (under different tags) that allow you to select an image that can answer your needs. Take a deeper look at the &lt;code&gt;Dockerfile&lt;/code&gt; files and &lt;code&gt;mvn-entrypoint.sh&lt;/code&gt; shell scripts when selecting Maven image to use.&lt;/p&gt;
&lt;p&gt;There are two flavors of official Maven Docker images: regular images (JDK version, Maven version, and Linux distro) and &lt;code&gt;onbuild&lt;/code&gt; images.&lt;/p&gt;
&lt;h5 id=&quot;what-is-the-official-maven-image-good-for&quot;&gt;What is the official Maven image good for?&lt;/h5&gt;
&lt;p&gt;The official Maven image does a good job containerizing the Maven tool itself. The image contains some JDK and Maven version. Using such image, you can run Maven build on any machine without installing a JDK and Maven.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; running &lt;code&gt;mvn clean install&lt;/code&gt; on local folder&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; my-maven-project &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;:/usr/src/app &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; /usr/src/app maven:3.2-jdk-7 mvn clean &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Maven local repository, for official Maven images, is placed inside a Docker &lt;em&gt;data volume&lt;/em&gt;. That means, all downloaded dependencies &lt;strong&gt;are not part of the image&lt;/strong&gt; and &lt;strong&gt;will disappear&lt;/strong&gt; once the Maven container is destroyed. If you do not want to download dependencies on every build, mount Maven repository Docker volume to some persistent storage (at least local folder on the Docker host).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; running &lt;code&gt;mvn clean install&lt;/code&gt; on local folder with properly mounted Maven local repository&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; my-maven-project &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;:/usr/src/app &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;/.m2:/root/.m2 &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; /usr/src/app maven:3.2-jdk-7 mvn clean &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now, let&apos;s take a look at onbuild Maven Docker images.&lt;/p&gt;
&lt;h5 id=&quot;what-is-maven-codeonbuildcode-image&quot;&gt;What is Maven &lt;code&gt;onbuild&lt;/code&gt; image?&lt;/h5&gt;
&lt;p&gt;Maven &lt;code&gt;onbuild&lt;/code&gt; Docker image exists to &lt;em&gt;‚Äúsimplify‚Äù&lt;/em&gt; developer‚Äôs life, allowing him/er skip writing a &lt;code&gt;Dockerfile&lt;/code&gt;. Actually, a developer should write a &lt;code&gt;Dockerfile&lt;/code&gt;, but it‚Äôs usually enough to have the single line in it:&lt;/p&gt;
&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; maven:&amp;lt;versions&amp;gt;-onbuild&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Looking into onbuild Dockerfile on the GitHub repository ‚Ä¶&lt;/p&gt;
&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; maven:&amp;lt;version&amp;gt;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/src/app
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /usr/src/app&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;ONBUILD&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;ADD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; . /usr/src/app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ONBUILD RUN &lt;/span&gt;mvn &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;‚Ä¶ you can see several &lt;code&gt;Dockerfile&lt;/code&gt; commands with the ONBUILD prefix. The &lt;code&gt;ONBUILD&lt;/code&gt; tells Docker to postpone the execution of these build commands until building a new image that inherits from the current image.&lt;/p&gt;
&lt;p&gt;In our example, two build commands will be executed, when you build the application &lt;code&gt;Dockerfile&lt;/code&gt; created &lt;code&gt;FROM: maven:&amp;lt;version&amp;gt;-onbuild&lt;/code&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add current folder (all files, if you are not using &lt;code&gt;.dockerignore&lt;/code&gt;) to the new Docker image&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;mvn install&lt;/code&gt; Maven target&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;onbuild&lt;/code&gt; Maven Docker image is not as useful as the previous image.&lt;/p&gt;
&lt;p&gt;First of all, it copies everything from the current repository, so do not use it without a properly configured &lt;code&gt;.dockerignore&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Then, think: &lt;em&gt;what kind of image you are trying to build?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The new image, created from &lt;code&gt;onbuild&lt;/code&gt;  Maven Docker image, includes JDK, Maven, application code (and potentially &lt;strong&gt;all files&lt;/strong&gt; from current directory), and &lt;strong&gt;all files&lt;/strong&gt; produced by Maven &lt;code&gt;install&lt;/code&gt; phase (compiled, tested and packaged app; plus lots of build junk files you do not really need).&lt;/p&gt;
&lt;p&gt;So, this Docker image contains everything, but, for some strange reason, does not contain a local Maven repository. I have no idea why the Maven team created this image.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Recommendation:&lt;/strong&gt; Do not use Maven onbuild images!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you just want to use Maven tool, use non-onbuild image.&lt;/p&gt;
&lt;p&gt;If you want to create proper Builder image, I will show you how to do this later in this post.&lt;/p&gt;
&lt;h4 id=&quot;where-to-keep-maven-cache&quot;&gt;Where to keep Maven cache?&lt;/h4&gt;
&lt;p&gt;Official Maven Docker image chooses to keep Maven cache folder outside of the container, exposing it as a Docker &lt;em&gt;data volume&lt;/em&gt;, using &lt;code&gt;VOLUME root/.m2&lt;/code&gt; command in the &lt;code&gt;Dockerfile&lt;/code&gt;. A Docker data volume is a directory within one or more containers that bypasses the Docker Union File System, in simple words: it‚Äôs not part of the Docker image.&lt;/p&gt;
&lt;p&gt;What you should know about Docker &lt;em&gt;data volumes&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Volumes are initialized when a container is created.&lt;/li&gt;
&lt;li&gt;Data volumes can be shared and reused among containers.&lt;/li&gt;
&lt;li&gt;Changes to a data volume are made directly to the mounted endpoint (usually some directory on host, but can be some storage device too)&lt;/li&gt;
&lt;li&gt;Changes to a data volume will not be included when you update an image or persist Docker container.&lt;/li&gt;
&lt;li&gt;Data volumes persist even if the container itself is deleted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, in order to &lt;em&gt;reuse&lt;/em&gt; Maven &lt;em&gt;cache&lt;/em&gt; between different builds, mount a Maven &lt;em&gt;cache data volume&lt;/em&gt; to some persistent storage (for example, a local directory on the Docker host).&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;/pom.xml://usr/src/app/pom.xml &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;/.m2:/root/.m2 maven:3-jdk-8-alpine mvn &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The command above runs the official Maven Docker image (Maven 3 and OpenJDK 8), mounts project &lt;code&gt;pom.xml&lt;/code&gt; file into working directory and &lt;code&gt;$HOME&amp;quot;/.m2&lt;/code&gt; folder for Maven &lt;em&gt;cache data volume&lt;/em&gt;. Maven running inside this Docker container will download all required JAR files into host‚Äôs local&lt;/p&gt;
&lt;p&gt;Maven running inside this Docker container will download all required &lt;code&gt;JAR&lt;/code&gt; files into host‚Äôs local folder &lt;code&gt;$HOME/.m2&lt;/code&gt;. Next time you create new Maven Docker container for the same &lt;code&gt;pom.xml&lt;/code&gt; file and the same &lt;em&gt;cache&lt;/em&gt; mount, Maven will reuse the &lt;em&gt;cache&lt;/em&gt; and will download only missing or updated &lt;code&gt;JAR&lt;/code&gt; files.&lt;/p&gt;
&lt;h4 id=&quot;maven-builder-docker-image&quot;&gt;Maven Builder Docker image&lt;/h4&gt;
&lt;p&gt;First, let‚Äôs try to formulate &lt;em&gt;what is the &lt;strong&gt;Builder&lt;/strong&gt; Docker image and what should it contain?&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Builder&lt;/strong&gt; is a Docker image that contains &lt;strong&gt;everything&lt;/strong&gt; to allow you creating a reproducible build on any machine and at any point of time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, &lt;em&gt;what should it contain?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux shell and some tools - I prefer Alpine Linux&lt;/li&gt;
&lt;li&gt;JDK (version) - for the &lt;code&gt;javac&lt;/code&gt; compiler&lt;/li&gt;
&lt;li&gt;Maven (version) - Java build tool&lt;/li&gt;
&lt;li&gt;Application source code and &lt;code&gt;pom.xml&lt;/code&gt; file/s - it‚Äôs the application code &lt;code&gt;SNAPSHOT&lt;/code&gt; at specific point of time; just code, no need to include a &lt;code&gt;.git&lt;/code&gt; repository or other files&lt;/li&gt;
&lt;li&gt;Project dependencies (Maven local repository) - all &lt;code&gt;POM&lt;/code&gt; and &lt;code&gt;JAR&lt;/code&gt; files you need to build and test Java application, at any time, even offline, even if library disappear from the web&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;Builder&lt;/strong&gt; image captures code, dependencies, and tools at a specific point of time and stores them inside a Docker image. The &lt;strong&gt;Builder&lt;/strong&gt; container can be used to create the application ‚Äúbinaries‚Äù on any machine, at any time and even without internet connection (or with poor connection).&lt;/p&gt;
&lt;p&gt;Here is the sample &lt;code&gt;Dockerfile&lt;/code&gt; for my demo &lt;strong&gt;Builder&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; openjdk:8-jdk-alpine&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ----&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Install Maven&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apk add &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; curl &lt;span class=&quot;nb&quot;&gt;tar &lt;/span&gt;bash

&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; MAVEN_VERSION=3.3.9&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; USER_HOME_DIR=&quot;/root&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/share/maven &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;  curl &lt;span class=&quot;nt&quot;&gt;-fsSL&lt;/span&gt; http://apache.osuosl.org/maven/maven-3/&lt;span class=&quot;nv&quot;&gt;$MAVEN_VERSION&lt;/span&gt;/binaries/apache-maven-&lt;span class=&quot;nv&quot;&gt;$MAVEN_VERSION&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-bin&lt;/span&gt;.tar.gz | &lt;span class=&quot;nb&quot;&gt;tar&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-xzC&lt;/span&gt; /usr/share/maven &lt;span class=&quot;nt&quot;&gt;--strip-components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\
&lt;/span&gt;  &lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /usr/share/maven/bin/mvn /usr/bin/mvn

&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; MAVEN_HOME /usr/share/maven&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; MAVEN_CONFIG &quot;$USER_HOME_DIR/.m2&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# speed up Maven JVM a bit&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ENV&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; MAVEN_OPTS=&quot;-XX:+TieredCompilation -XX:TieredStopAtLevel=1&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;ENTRYPOINT&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;/usr/bin/mvn&quot;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ----&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Install project dependencies and keep sources &lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# make source folder&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/src/app
&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /usr/src/app&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# install maven dependency packages (keep in image)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; pom.xml /usr/src/app&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;mvn &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; 1C &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; target

&lt;span class=&quot;c&quot;&gt;# copy other source files (keep in image)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; src /usr/src/app/src&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let‚Äôs go over this &lt;code&gt;Dockerfile&lt;/code&gt; and I will try to explain the reasoning behind each command.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;FROM: openjdk:8-jdk-alpine&lt;/code&gt; - select and freeze JDK version: OpenJDK 8 and Linux Alpine&lt;/li&gt;
&lt;li&gt;Install Maven
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ARG ...&lt;/code&gt; - Use build arguments to allow overriding Maven version and local repository location (&lt;code&gt;MAVEN_VERSION&lt;/code&gt; and &lt;code&gt;USER_HOME_DIR&lt;/code&gt;) with &lt;code&gt;docker build --build-arg ...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RUN mkdir -p ... curl ... tar ...&lt;/code&gt; - Download and install (&lt;code&gt;untar&lt;/code&gt; and &lt;code&gt;ln -s&lt;/code&gt;) Apache Maven&lt;/li&gt;
&lt;li&gt;Speed up Maven JVM a bit: &lt;code&gt;MAVEN_OPTS=&amp;quot;-XX:+TieredCompilation -XX:TieredStopAtLevel=1&amp;quot;&lt;/code&gt;, read the following &lt;a href=&quot;https://zeroturnaround.com/rebellabs/your-maven-build-is-slow-speed-it-up/&quot;&gt;post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RUN mvn -T 1C install &amp;amp;&amp;amp; rm -rf target&lt;/code&gt; Download project dependencies:
&lt;ul&gt;
&lt;li&gt;Copy project &lt;code&gt;pom.xml&lt;/code&gt; file and run &lt;code&gt;mvn install&lt;/code&gt; command and remove build artifacts as far as I know, there is no Maven command that will let you download without installing)&lt;/li&gt;
&lt;li&gt;This Docker image layer will be rebuilt only when project‚Äôs &lt;code&gt;pom.xml&lt;/code&gt; file changes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;COPY src /usr/src/app/src&lt;/code&gt; - copy project source files (source, tests, and resources)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; if you are using &lt;a href=&quot;http://maven.apache.org/surefire/maven-surefire-plugin&quot;&gt;Maven Surefire plugin&lt;/a&gt; and want to have all dependencies for the offline build, make sure to &lt;a href=&quot;http://maven.apache.org/surefire/maven-surefire-plugin/examples/providers.html&quot;&gt;lock down Surefire test provider&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you build a new &lt;strong&gt;Builder&lt;/strong&gt; version, I suggest you use a &lt;code&gt;--cache-from&lt;/code&gt; option passing previous Builder image to it. This will allow you reuse any unmodified Docker layer and avoid obsolete downloads most of the time (if &lt;code&gt;pom.xml&lt;/code&gt; did not change or you did not decide to upgrade Maven or JDK).&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# pull latest (or specific version) builder image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker pull myrep/mvn-builder:latest
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# build new builder&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; myrep/mvn-builder:latest &lt;span class=&quot;nt&quot;&gt;--cache-from&lt;/span&gt; myrep/mvn-builder:latest &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;use-builder-container-to-run-tests&quot;&gt;Use Builder container to run tests&lt;/h5&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# run tests - test results are saved into $PWD/target/surefire-reports&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;/target:/usr/src/app/target myrep/mvn-builder &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; 1C &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;use-builder-container-to-create-application-war&quot;&gt;Use Builder container to create application WAR&lt;/h5&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# create application WAR file (skip tests) - $PWD/target/spring-boot-rest-example-0.3.0.war&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;shell &lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;/target:/usr/src/app/target myrep/mvn-builder package &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; 1C &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-Dmaven&lt;/span&gt;.test.skip&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Take a look at images bellow:&lt;/p&gt;
&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;REPOSITORY      TAG     IMAGE ID     CREATED        SIZE
sbdemo/run      latest  6f432638aa60 7 minutes ago  143 MB
sbdemo/tutorial 1       669333d13d71 12 minutes ago 1.28 GB
sbdemo/tutorial 2       38634e4d9d5e 3 hours ago    1.26 GB
sbdemo/builder  mvn     2d325a403c5f 5 days ago     263 MB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sbdemo/run:latest&lt;/code&gt; - Docker image for demo runtime: Alpine, OpenJDK JRE only, demo WAR&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sbdemo/builder:mvn&lt;/code&gt; - &lt;strong&gt;Builder&lt;/strong&gt; Docker image: Alpine, OpenJDK 8, Maven 3, code, dependency&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sbdemo/tutorial:1&lt;/code&gt; - Docker image created following first tutorial (just for reference)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sbdemo/tutorial:2&lt;/code&gt; - Docker image created following second tutorial (just for reference)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;bonus-build-flow-automation&quot;&gt;Bonus: Build flow automation&lt;/h2&gt;
&lt;p&gt;In this section, I will show how to use Docker build flow automation service to automate and orchestrate all steps from this post.&lt;/p&gt;
&lt;h3 id=&quot;build-pipeline-steps&quot;&gt;Build Pipeline Steps&lt;/h3&gt;
&lt;p&gt;I&apos;m going to use &lt;a href=&quot;https://codefresh.io&quot;&gt;Codefresh.io&lt;/a&gt; Docker CI/CD service (the company I&apos;m working for) to create a &lt;strong&gt;Builder&lt;/strong&gt; Docker image for Maven, run tests, create application WAR, build Docker image for application and deploy it to DockerHub.&lt;/p&gt;
&lt;p&gt;The Codefresh automation flow &lt;code&gt;YAML&lt;/code&gt; (also called &lt;em&gt;pipeline&lt;/em&gt;) is pretty straight forward:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it contains ordered list of steps&lt;/li&gt;
&lt;li&gt;each step can be of type:&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;build&lt;/code&gt; - for &lt;code&gt;docker build&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;push&lt;/code&gt; - for &lt;code&gt;docker push&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;composition&lt;/code&gt; - for creating environment, specified with &lt;code&gt;docker-compose&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;freestyle&lt;/code&gt; (default if not specified) - for &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/codefresh/volume/&lt;/code&gt; &lt;em&gt;data volume&lt;/em&gt; (&lt;code&gt;git clone&lt;/code&gt; and files generated by steps) is mounted into each step&lt;/li&gt;
&lt;li&gt;current working directory for each step is set to &lt;code&gt;/codefresh/volume/&lt;/code&gt; by default (can be changed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For detailed description and other examples, take a look at the &lt;a href=&quot;https://docs.codefresh.io/docs/steps&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For my demo flow I&apos;ve created following automation steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;mvn_builder&lt;/code&gt; - create Maven &lt;strong&gt;Builder&lt;/strong&gt; Docker image&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mv_test&lt;/code&gt; - execute tests in &lt;strong&gt;Builder&lt;/strong&gt; container, place test results into &lt;code&gt;/codefresh/volume/target/surefire-reports/&lt;/code&gt; &lt;em&gt;data volume&lt;/em&gt; folder&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mv_package&lt;/code&gt; - create application &lt;code&gt;WAR&lt;/code&gt; file, place created file into &lt;code&gt;/codefresh/volume/target/&lt;/code&gt; &lt;em&gt;data volume&lt;/em&gt; folder&lt;/li&gt;
&lt;li&gt;&lt;code&gt;build_image&lt;/code&gt; - build application Docker image with JRE and application &lt;code&gt;WAR&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;push_image&lt;/code&gt; - tag and push the application Docker image to DockerHub&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is the full Codefresh &lt;code&gt;YAML&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1.0&apos;&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;mvn_builder&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create Maven builder image&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;dockerfile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Dockerfile.build&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;put_you_repo_here&amp;gt;/mvn-builder&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;mvn_test&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;run unit tests&lt;/span&gt; 
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mvn -T 1C -o test&lt;/span&gt;
  
  &lt;span class=&quot;na&quot;&gt;mvn_package&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;package application and dependencies into WAR&lt;/span&gt; 
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;commands&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mvn package -T 1C -o -Dmaven.test.skip=true&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;build_image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;create Docker image with application WAR&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;dockerfile&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Dockerfile&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;working_directory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$/target&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;put_you_repo_here&amp;gt;/sbdemo&lt;/span&gt;

  &lt;span class=&quot;na&quot;&gt;push_image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;push&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;push application image to DockerHub&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;candidate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&apos;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;# set docker registry credentials in project configuration&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&apos;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;Hope, you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;This is a &lt;strong&gt;working draft&lt;/strong&gt; version. The final post version is published at &lt;a href=&quot;https://codefresh.io/blog/java_docker_pipeline/&quot;&gt;Codefresh Blog&lt;/a&gt; on March 22, 2017.&lt;/em&gt;&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="DevOps" /><category term="docker" /><category term="tutorial" /><category term="devops" /><category term="hacks" /><category term="java" /><category term="maven" /><summary type="html">TL;DR</summary></entry><entry><title type="html">Everyday hacks for Docker</title><link href="http://localhost:4000/devops/2017/01/02/everyday-hacks.html" rel="alternate" type="text/html" title="Everyday hacks for Docker" /><published>2017-01-02T18:00:00+02:00</published><updated>2017-01-02T18:00:00+02:00</updated><id>http://localhost:4000/devops/2017/01/02/everyday-hacks</id><content type="html" xml:base="http://localhost:4000/devops/2017/01/02/everyday-hacks.html">&lt;p&gt;In this post, I&apos;ve decided to share with you some useful commands and tools, I&apos;m frequently using, working with amazing Docker technology.
There is no particular order or &amp;quot;coolness level&amp;quot; for every &amp;quot;hack&amp;quot;. I will try to present the use case and how does specific command or tool help me with my work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/docker_animals.png&quot; alt=&quot;Docker Hacks&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;cleaning-up&quot;&gt;Cleaning up&lt;/h2&gt;
&lt;p&gt;Working with Docker for some time, you start to accumulate development junk: unused volumes, networks, exited containers and unused images.&lt;/p&gt;
&lt;h3 id=&quot;one-command-to-quotrule-them-allquot&quot;&gt;One command to &amp;quot;rule them all&amp;quot;&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ docker system  prune
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;prune&lt;/code&gt; is a very useful command (works also for &lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; sub-commands), but it&apos;s only available for Docker 1.13. So, if you are using older Docker versions, then following commands can help you to replace the &lt;code&gt;prune&lt;/code&gt; command.&lt;/p&gt;
&lt;h3 id=&quot;remove-dangling-volumes&quot;&gt;Remove dangling volumes&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;dangling&lt;/code&gt; volumes - volumes not in use by any container. To remove them, combine two commands: first, list volume IDs for &lt;code&gt;dangling&lt;/code&gt; volumes and then remove them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker volume rm $(docker volume ls -q -f &amp;quot;dangling=true&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;remove-exited-containers&quot;&gt;Remove exited containers&lt;/h3&gt;
&lt;p&gt;The same principle works here too: first, list containers (only IDs) you want to remove (with filter) and then remove them (consider &lt;code&gt;rm -f&lt;/code&gt; to force remove).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker rm $(docker ps -q -f &amp;quot;status=exited&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;remove-dangling-images&quot;&gt;Remove dangling images&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;dangling&lt;/code&gt; images are Docker untagged images, that are the leaves of the images tree (not intermediary layers).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker rmi $(docker images -q -f &amp;quot;dangling=true&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;autoremove-interactive-containers&quot;&gt;Autoremove interactive containers&lt;/h3&gt;
&lt;p&gt;When you run a new interactive container and want to avoid typing &lt;code&gt;rm&lt;/code&gt; command after it exits, use &lt;code&gt;--rm&lt;/code&gt; option. Then when you exit from created container, it will be automatically destroyed.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -it --rm alpine sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;inspect-docker-resources&quot;&gt;Inspect Docker resources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; - &lt;code&gt;jq&lt;/code&gt; is a lightweight and flexible command-line &lt;code&gt;JSON&lt;/code&gt; processor. It is like &lt;code&gt;sed&lt;/code&gt; for JSON data - you can use it to slice and filter and map and transform structured data with the same ease that &lt;code&gt;sed&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt; and friends let you play with text.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/info/&quot;&gt;&lt;code&gt;docker info&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/inspect/&quot;&gt;&lt;code&gt;docker inspect&lt;/code&gt;&lt;/a&gt; commands can produce output in &lt;code&gt;JSON&lt;/code&gt; format. Combine these commands with &lt;code&gt;jq&lt;/code&gt; processor.&lt;/p&gt;
&lt;h3 id=&quot;pretty-json-and-jq-processing&quot;&gt;Pretty JSON and jq processing&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# show whole Docker info
$ docker info --format &apos;\{\{json .\}\}&apos; | jq .

# show Plugins only
$ docker info --format &apos;\{\{json .Plugins\}\}&apos; | jq .

# list IP addresses for all containers connected to &apos;bridge&apos; network
$ docker network inspect bridge -f &apos;\{\{json .Containers\}\}&apos; | jq &apos;.[] | {cont: .Name, ip: .IPv4Address}&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;watching-containers-lifecycle&quot;&gt;Watching containers lifecycle&lt;/h2&gt;
&lt;p&gt;Sometimes you want to see containers being activated and exited, when you run some docker commands or try different restart policies.
&lt;a href=&quot;http://man7.org/linux/man-pages/man1/watch.1.html&quot;&gt;&lt;code&gt;watch&lt;/code&gt;&lt;/a&gt; command combined with &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/ps/&quot;&gt;&lt;code&gt;docker ps&lt;/code&gt;&lt;/a&gt; can be pretty useful here.
The &lt;code&gt;docker stats&lt;/code&gt; command, even with &lt;code&gt;--format&lt;/code&gt; option is not useful for this use case since it does not allow you to see same info as you can see with &lt;code&gt;docker ps&lt;/code&gt; command.&lt;/p&gt;
&lt;h3 id=&quot;display-a-table-with-id-image-status-for-active-containers-and-refresh-it-every-2-seconds&quot;&gt;Display a table with &apos;ID Image Status&apos; for active containers and refresh it every 2 seconds&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ watch -n 2 &apos;docker ps --format &amp;quot;table \{\{.ID\}\}\t \{\{.Image\}\}\t \{\{.Status\}\}&amp;quot;&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;enter-into-hostcontainer-namespace&quot;&gt;Enter into host/container Namespace&lt;/h2&gt;
&lt;p&gt;Sometimes you want to connect to the Docker host. The &lt;code&gt;ssh&lt;/code&gt; command is the default option, but this option can be either not available, due to security settings, firewall rules or not documented (try to find how to &lt;code&gt;ssh&lt;/code&gt; into Docker for Mac VM).&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/jpetazzo/nsenter&quot;&gt;&lt;code&gt;nsenter&lt;/code&gt;&lt;/a&gt;, by J√©r√¥me Petazzoni, is a small and very useful tool for above cases. &lt;code&gt;nsenter&lt;/code&gt; allows to &lt;code&gt;enter&lt;/code&gt; into &lt;code&gt;n&lt;/code&gt;ame&lt;code&gt;s&lt;/code&gt;paces. I like to use minimalistic (&lt;code&gt;580 kB&lt;/code&gt;) &lt;a href=&quot;https://hub.docker.com/r/walkerlee/nsenter/&quot;&gt;walkerlee/nsenter&lt;/a&gt; Docker image.&lt;/p&gt;
&lt;h3 id=&quot;enter-into-docker-host&quot;&gt;Enter into Docker host&lt;/h3&gt;
&lt;p&gt;Use &lt;code&gt;--pid=host&lt;/code&gt; to enter into Docker host namespaces&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# get a shell into Docker host
$ docker run --rm -it --privileged --pid=host walkerlee/nsenter -t 1 -m -u -i -n sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;enter-into-any-container&quot;&gt;Enter into ANY container&lt;/h3&gt;
&lt;p&gt;It&apos;s also possible to enter into any container with &lt;code&gt;nsenter&lt;/code&gt; and &lt;code&gt;--pid=container:[id OR name]&lt;/code&gt;. But in most cases, it&apos;s better to use standard &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/exec/&quot;&gt;&lt;code&gt;docker exec&lt;/code&gt;&lt;/a&gt; command. The main difference is that &lt;code&gt;nsenter&lt;/code&gt; doesn&apos;t enter the &lt;em&gt;cgroups&lt;/em&gt;, and therefore evades resource limitations (can be useful for debugging).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# get a shell into &apos;redis&apos; container namespace
$ docker run --rm -it --privileged --pid=container:redis walkerlee/nsenter -t 1 -m -u -i -n sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;heredoc-docker-container&quot;&gt;Heredoc Docker container&lt;/h2&gt;
&lt;p&gt;Suppose you want to get some tool as a Docker image, but you do not want to search for a suitable image or to create a new &lt;code&gt;Dockerfile&lt;/code&gt; (no need to keep it for future use, for example). Sometimes storing a Docker image definition in a file looks like an overkill - you need to decide how do you edit, store and share this Dockerfile. Sometimes it&apos;s better just to have a single line command, that you can copy, share, embed into a shell script or create special command &lt;code&gt;alias&lt;/code&gt;.
So, when you want to create a new ad-hoc container with a single command, try a &lt;a href=&quot;http://www.tldp.org/LDP/abs/html/here-docs.html&quot;&gt;Heredoc&lt;/a&gt; approach.&lt;/p&gt;
&lt;h3 id=&quot;create-alpine-based-container-with-htop-tool&quot;&gt;Create Alpine based container with &apos;htop&apos; tool&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ docker build -t htop - &amp;lt;&amp;lt; EOF
FROM alpine
RUN apk --no-cache add htop
EOF
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;docker-command-completion&quot;&gt;Docker command completion&lt;/h2&gt;
&lt;p&gt;Docker CLI syntax is very rich and constantly growing: adding new commands and new options. It&apos;s hard to remember every possible command and option, so having a nice command completion for a terminal is a &lt;strong&gt;must have&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Command completion is a kind of terminal plugin, that lets you auto-complete or auto-suggest what to type in next by hitting &lt;em&gt;tab&lt;/em&gt; key. Docker command completion works both for commands and options. Docker team prepared command completion for &lt;code&gt;docker&lt;/code&gt;, &lt;code&gt;docker-machine&lt;/code&gt; and &lt;code&gt;docker-compose&lt;/code&gt; commands, both for &lt;code&gt;Bash&lt;/code&gt; and &lt;code&gt;Zsh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you are using Mac and &lt;a href=&quot;http://brew.sh&quot;&gt;Homebrew&lt;/a&gt;, then installing Docker commands completion is pretty straight forward.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Tap homebrew/completion to gain access to these
$ brew tap homebrew/completions

# Install completions for docker suite
$ brew install docker-completion
$ brew install docker-compose-completion
$ brew install docker-machine-completion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For non-Mac install read official Docker documentation: &lt;a href=&quot;https://github.com/docker/docker/tree/master/contrib/completion&quot;&gt;docker engine&lt;/a&gt;, &lt;a href=&quot;https://docs.docker.com/compose/completion/&quot;&gt;docker-compose&lt;/a&gt; and &lt;a href=&quot;https://docs.docker.com/machine/completion/&quot;&gt;docker-machine&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;start-containers-automatically&quot;&gt;Start containers automatically&lt;/h2&gt;
&lt;p&gt;When you are running process in Docker container, it may fail due to multiple reasons. Sometimes to fix this failure it&apos;s enough to rerun the failed container. If you are using Docker orchestration engine, like Swarm or Kubernetes, the failed service will be restarted automatically.
But if you are using plain Docker and want to restart container, based on &lt;em&gt;exit code&lt;/em&gt; of container&apos;s main process or always (regardless the &lt;em&gt;exit code&lt;/em&gt;), Docker 1.12 introduced a very helpful option for &lt;code&gt;docker run&lt;/code&gt; command: &lt;a href=&quot;https://docs.docker.com/engine/reference/run/#restart-policies-restart#restart-policies---restart&quot;&gt;restart&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;restart-always&quot;&gt;Restart always&lt;/h3&gt;
&lt;p&gt;Restart the &lt;code&gt;redis&lt;/code&gt; container with a restart policy of &lt;strong&gt;always&lt;/strong&gt; so that if the container exits, Docker will restart it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run --restart=always redis
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;restart-container-on-failure&quot;&gt;Restart container on failure&lt;/h3&gt;
&lt;p&gt;Restart the &lt;code&gt;redis&lt;/code&gt; container with a restart policy of &lt;strong&gt;on-failure&lt;/strong&gt; and a maximum restart count of &lt;code&gt;10&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run --restart=on-failure:10 redis
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;network-tricks&quot;&gt;Network tricks&lt;/h2&gt;
&lt;p&gt;There are cases when you want to create a new container and connect it to already existing network stack. It can be Docker host network or another container&apos;s network. This can be pretty useful for debugging and audition network issues.
The &lt;code&gt;docker run --network/net&lt;/code&gt; option support this use case.&lt;/p&gt;
&lt;h3 id=&quot;use-docker-host-network-stack&quot;&gt;Use Docker host network stack&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ docker run --net=host ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new container will attach to same network interfaces as the Docker host.&lt;/p&gt;
&lt;h3 id=&quot;use-another-containers-network-stack&quot;&gt;Use another container&apos;s network stack&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;$ docker run --net=container:&amp;lt;name|id&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new container will attach to same network interfaces as another container. The target container can be specified with &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;name&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;attachable-overlay-network&quot;&gt;Attachable overlay network&lt;/h3&gt;
&lt;p&gt;Using docker engine running in &lt;strong&gt;swarm mode&lt;/strong&gt;, you can create a multi-host &lt;code&gt;overlay&lt;/code&gt; network on a manager node. When you create a new &lt;em&gt;swarm service&lt;/em&gt;, you can attach it to the previously created &lt;code&gt;overlay&lt;/code&gt; network.&lt;/p&gt;
&lt;p&gt;Sometimes to inspect network configuration or debug network issues, you want to attach a new Docker container, filled with different network tools, to existing &lt;code&gt;overlay&lt;/code&gt; network and do this with &lt;code&gt;docker run&lt;/code&gt; command and not to create a new &amp;quot;debug service&amp;quot;.&lt;/p&gt;
&lt;p&gt;Docker 1.13 brings a new option to &lt;code&gt;docker network create&lt;/code&gt; command: &lt;code&gt;attachable&lt;/code&gt;. The &lt;code&gt;attachable&lt;/code&gt; option enables manual container attachment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# create an attachable overlay network
$ docker network create --driver overlay --attachable mynet
# create net-tools container and attach it to mynet overlay network
$ docker run -it --rm --net=mynet net-tools sh
&lt;/code&gt;&lt;/pre&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="DevOps" /><category term="docker" /><category term="tutorial" /><category term="devops" /><category term="hacks" /><summary type="html">In this post, I&apos;ve decided to share with you some useful commands and tools, I&apos;m frequently using, working with amazing Docker technology. There is no particular order or &amp;quot;coolness level&amp;quot; for every &amp;quot;hack&amp;quot;. I will try to present the use case and how does specific command or tool help me with my work.</summary></entry><entry><title type="html">Deploy Docker Compose (v3) to Swarm (mode) Cluster</title><link href="http://localhost:4000/devops/2016/12/18/composev3-swarm.html" rel="alternate" type="text/html" title="Deploy Docker Compose (v3) to Swarm (mode) Cluster" /><published>2016-12-18T14:00:00+02:00</published><updated>2016-12-18T14:00:00+02:00</updated><id>http://localhost:4000/devops/2016/12/18/composev3-swarm</id><content type="html" xml:base="http://localhost:4000/devops/2016/12/18/composev3-swarm.html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; all code snippets bellow are working only with &lt;strong&gt;Docker 1.13+&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Docker 1.13&lt;/strong&gt; simplifies deployment of composed application to a &lt;strong&gt;swarm&lt;/strong&gt; (mode) cluster. And you can do it without creating a new &lt;code&gt;dab&lt;/code&gt; (&lt;em&gt;Distribution Application Bundle&lt;/em&gt;) file, but just using familiar and well-known &lt;code&gt;docker-compose.yml&lt;/code&gt; syntax (with some additions) and &lt;code&gt;--compose-file&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/compose_swarm.png&quot; alt=&quot;Compose to Swarm&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;swarm-cluster&quot;&gt;Swarm cluster&lt;/h1&gt;
&lt;p&gt;Docker Engine 1.12 introduced a new &lt;strong&gt;swarm mode&lt;/strong&gt; for natively managing a cluster of Docker Engines called a &lt;strong&gt;swarm&lt;/strong&gt;. Docker &lt;strong&gt;swarm mode&lt;/strong&gt; implements &lt;a href=&quot;https://docs.docker.com/engine/swarm/raft/&quot;&gt;Raft Consensus Algorithm&lt;/a&gt; and does not require using external key value store anymore, such as &lt;a href=&quot;https://www.consul.io/&quot;&gt;Consul&lt;/a&gt; or &lt;a href=&quot;https://github.com/coreos/etcd&quot;&gt;etcd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to run a &lt;strong&gt;swarm&lt;/strong&gt; cluster on a developer&apos;s machine, there are several options.&lt;/p&gt;
&lt;p&gt;The first option and most widely known, is to use a &lt;code&gt;docker-machine&lt;/code&gt; tool with some virtual driver (Virtualbox, Parallels or other).&lt;/p&gt;
&lt;p&gt;But, in this post I will use another approach: using &lt;a href=&quot;https://hub.docker.com/_/docker/&quot;&gt;docker-in-docker&lt;/a&gt; Docker image with Docker for Mac, see more details in my &lt;a href=&quot;../swarm_dind&quot;&gt;Docker Swarm cluster with docker-in-docker on MacOS&lt;/a&gt; post.&lt;/p&gt;
&lt;h2 id=&quot;docker-registry-mirror&quot;&gt;Docker Registry mirror&lt;/h2&gt;
&lt;p&gt;When you deploy a new service on local swarm cluster, I recommend to setup local Docker registry mirror and run all swarm nodes with &lt;code&gt;--registry-mirror&lt;/code&gt; option, pointing to local Docker registry. By running a local Docker registry mirror, you can keep most of the redundant image fetch traffic on your local network and speedup service deployment.&lt;/p&gt;
&lt;h3 id=&quot;docker-swarm-cluster-bootstrap-script&quot;&gt;Docker Swarm cluster bootstrap script&lt;/h3&gt;
&lt;p&gt;I&apos;ve prepared a shell script to bootstrap 4 nodes swarm cluster with Docker registry mirror and very nice &lt;a href=&quot;https://github.com/ManoMarks/docker-swarm-visualizer&quot;&gt;swarm visualizer&lt;/a&gt; application.&lt;/p&gt;
&lt;p&gt;The script initialize docker engine as a &lt;strong&gt;swarm master&lt;/strong&gt;, then starts 3 new docker-in-docker containers and join them to the &lt;strong&gt;swarm&lt;/strong&gt; cluster as worker nodes. All worker nodes run with &lt;code&gt;--registry-mirror&lt;/code&gt; option.&lt;/p&gt;
&amp;lt;script src=&quot;https://gist.github.com/alexei-led/a4d31ee446a0fbcab845b93fe4a9b09d.js?file=create_swarm_cluster.sh&quot;&gt; &lt;/script&gt;
&lt;h2 id=&quot;deploy-multi-container-application---the-quotoldquot-way&quot;&gt;Deploy multi-container application - the &amp;quot;old&amp;quot; way&lt;/h2&gt;
&lt;p&gt;The Docker &lt;code&gt;compose&lt;/code&gt; is a tool (and deployment specification format) for defining and running composed multi-container Docker applications. Before Docker 1.12, you could use &lt;code&gt;docker-compose&lt;/code&gt; tool to deploy such applications to a &lt;strong&gt;swarm&lt;/strong&gt; cluster. With 1.12 release, it&apos;s not possible anymore: &lt;code&gt;docker-compose&lt;/code&gt; can deploy your application only on single Docker host.&lt;/p&gt;
&lt;p&gt;In order to deploy it to a &lt;strong&gt;swarm&lt;/strong&gt; cluster, you need to create a special deployment specification file (also knows as &lt;em&gt;Distribution Application Bundle&lt;/em&gt;) in &lt;code&gt;dab&lt;/code&gt; format (see more &lt;a href=&quot;https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The way to create this file, is to run the &lt;code&gt;docker-compose bundle&lt;/code&gt; command. The output of this command is a JSON file, that describes  multi-container composed application with Docker images referenced by &lt;code&gt;@sha256&lt;/code&gt; instead of tags. Currently &lt;code&gt;dab&lt;/code&gt; file format does not support multiple settings from &lt;code&gt;docker-compose.yml&lt;/code&gt; and does not allow to use supported options from &lt;code&gt;docker service create&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Such a pity story: the &lt;code&gt;dab&lt;/code&gt; bundle format looks promising, but currently is totally useless (at least in Docker 1.12).&lt;/p&gt;
&lt;h2 id=&quot;deploy-multi-container-application---the-quotnewquot-way&quot;&gt;Deploy multi-container application - the &amp;quot;new&amp;quot; way&lt;/h2&gt;
&lt;p&gt;With Docker 1.13, the &amp;quot;new&amp;quot; way to deploy a multi-container composed application is to use &lt;code&gt;docker-compose.yml&lt;/code&gt; again (&lt;em&gt;hurrah!&lt;/em&gt;). Kudos to Docker team!&lt;/p&gt;
&lt;p&gt;*&lt;strong&gt;Note&lt;/strong&gt;: And you do not need the &lt;code&gt;docker-compose&lt;/code&gt; tool, only &lt;code&gt;yaml&lt;/code&gt; file in &lt;strong&gt;docker-compose&lt;/strong&gt; format (&lt;code&gt;version: &amp;quot;3&amp;quot;&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker deploy --compose-file docker-compose.yml myapp
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;docker-compose-v3-codeversion-quot3quotcode&quot;&gt;Docker compose v3 (&lt;code&gt;version: &amp;quot;3&amp;quot;&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;So, what&apos;s new in docker compose version 3?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First, I suggest you take a deeper look at &lt;a href=&quot;https://github.com/aanand/compose-file/blob/master/schema/data/config_schema_v3.0.json&quot;&gt;docker-compose schema&lt;/a&gt;. It is an extension of well-known &lt;code&gt;docker-compose&lt;/code&gt; format.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;docker-compose&lt;/code&gt; tool (&lt;code&gt;ver. 1.9.0&lt;/code&gt;) does not support &lt;code&gt;docker-compose.yaml version: &amp;quot;3&amp;quot;&lt;/code&gt; yet.&lt;/p&gt;
&lt;p&gt;The most visible change is around &lt;strong&gt;swarm&lt;/strong&gt; ***service deployment.
Now you can specify all options supported by &lt;code&gt;docker service create/update&lt;/code&gt; commands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of service replicas (or global service)&lt;/li&gt;
&lt;li&gt;service labels&lt;/li&gt;
&lt;li&gt;hard and soft limits for service (container) CPU and memory&lt;/li&gt;
&lt;li&gt;service restart policy&lt;/li&gt;
&lt;li&gt;service rolling update policy&lt;/li&gt;
&lt;li&gt;deployment placement constraints &lt;a href=&quot;https://github.com/docker/docker/blob/master/docs/reference/commandline/service_create.md#specify-service-constraints---constraint&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;docker-compose-v3-example&quot;&gt;Docker compose v3 example&lt;/h3&gt;
&lt;p&gt;I&apos;ve created a &amp;quot;new&amp;quot; compose file (v3) for classic &amp;quot;Cats vs. Dogs&amp;quot; example. This example application contains 5 services with following deployment configurations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;voting-app&lt;/code&gt; - a Python webapp which lets you vote between two options; requires &lt;code&gt;redis&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;redis&lt;/code&gt; - Redis queue which collects new votes; deployed on &lt;code&gt;swarm manager&lt;/code&gt; node&lt;/li&gt;
&lt;li&gt;&lt;code&gt;worker&lt;/code&gt; .NET worker which consumes votes and stores them in &lt;code&gt;db&lt;/code&gt;;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;# of replicas:&lt;/strong&gt; 2 replicas&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hard limit:&lt;/strong&gt; max 25% CPU and 512MB memory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;soft limit:&lt;/strong&gt; max 25% CPU and 256MB memory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;placement:&lt;/strong&gt; on &lt;code&gt;swarm worker&lt;/code&gt; nodes only&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;restart policy:&lt;/strong&gt; restart on-failure, with 5 seconds delay, up to 3 attempts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update policy:&lt;/strong&gt; one by one, with 10 seconds delay and 0.3 failure rate to tolerate during the update&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;&lt;code&gt;db&lt;/code&gt; - Postgres database backed by a Docker volume; deployed on &lt;code&gt;swarm manager&lt;/code&gt; node&lt;/li&gt;
&lt;li&gt;&lt;code&gt;result-app&lt;/code&gt; Node.js webapp which shows the results of the voting in real time; 2 replicas, deployed on &lt;code&gt;swarm worker&lt;/code&gt; nodes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Run the &lt;code&gt;docker deploy --compose-file docker-compose.yml&lt;/code&gt; command to deploy my version of &amp;quot;Cats vs. Dogs&amp;quot; application on a swarm cluster.&lt;/p&gt;
&amp;lt;script src=&quot;https://gist.github.com/alexei-led/a4d31ee446a0fbcab845b93fe4a9b09d.js?file=docker-compose.yml&quot;&gt; &lt;/script&gt;
&lt;hr /&gt;
&lt;p&gt;Hope you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="DevOps" /><category term="docker" /><category term="swarm" /><category term="docker-compose" /><category term="compose" /><category term="devops" /><category term="cluster" /><summary type="html">Disclaimer: all code snippets bellow are working only with Docker 1.13+</summary></entry><entry><title type="html">Do not ignore .dockerignore</title><link href="http://localhost:4000/development/2016/11/26/donot-ignore-dockerignore.html" rel="alternate" type="text/html" title="Do not ignore .dockerignore" /><published>2016-11-26T16:00:00+02:00</published><updated>2016-11-26T16:00:00+02:00</updated><id>http://localhost:4000/development/2016/11/26/donot-ignore-dockerignore</id><content type="html" xml:base="http://localhost:4000/development/2016/11/26/donot-ignore-dockerignore.html">&lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; Consider to define and use &lt;code&gt;.dockerignore&lt;/code&gt; file for every Docker image you are building. It can help you to reduce Docker image size, speedup &lt;code&gt;docker build&lt;/code&gt; and avoid unintended secret exposure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/overloaded.jpg&quot; alt=&quot;Overloaded container ship&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;docker-build-context&quot;&gt;Docker build context&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;docker build&lt;/code&gt; command is used to build a new Docker image. There is one argument you can pass to the &lt;code&gt;build&lt;/code&gt; command &lt;strong&gt;build context&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, what is the Docker &lt;strong&gt;build context&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;First, remember, that Docker is a client-server application, it consists from Docker client and Docker server (also known as &lt;em&gt;daemon&lt;/em&gt;). The Docker client command line tool talks with Docker server and asks it do things. One of these things is &lt;strong&gt;build&lt;/strong&gt;: building a new Docker image. The Docker server can run on the same machine as the client, remote machine or virtual machine, that also can be local, remote or even run on some cloud IaaS.&lt;/p&gt;
&lt;p&gt;Why is that important and how is the Docker &lt;strong&gt;build context&lt;/strong&gt; related to this fact?&lt;/p&gt;
&lt;p&gt;In order to create a new Docker image, Docker server needs an access to files, you want to create the Docker image from. So, you need somehow to send these files to the Docker server. These files are the Docker &lt;strong&gt;build context&lt;/strong&gt;. The Docker client packs all &lt;strong&gt;build context&lt;/strong&gt; files into &lt;code&gt;tar&lt;/code&gt; archive and uploads this archive to the Docker server. By default client will take all files (and folders) in current working directory and use them as the &lt;strong&gt;build context&lt;/strong&gt;.
It can also accept already created &lt;code&gt;tar&lt;/code&gt; archive or &lt;code&gt;git&lt;/code&gt; repository. In a case of &lt;code&gt;git&lt;/code&gt; repository, the client will clone it with submodules into a temporary folder and will create a &lt;strong&gt;build context&lt;/strong&gt; archive from it.&lt;/p&gt;
&lt;h1 id=&quot;impact-on-docker-build&quot;&gt;Impact on Docker build&lt;/h1&gt;
&lt;p&gt;The first output line, that you see, running the &lt;code&gt;docker build&lt;/code&gt; command is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sending build context to Docker daemon 45.3 MB
Step 1: FROM ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should make things clear. Actually, &lt;strong&gt;every time&lt;/strong&gt; you are running the &lt;code&gt;docker build&lt;/code&gt; command, the Docker client creates a new &lt;strong&gt;build context&lt;/strong&gt; archive and sends it to the Docker server. So, you are always paying this &amp;quot;tax&amp;quot;: the time it takes to create an archive, storage and network traffic and latency time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The &lt;strong&gt;rule of thumb&lt;/strong&gt; is not adding files to the &lt;strong&gt;build context&lt;/strong&gt;, if you do not need them in your Docker image.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;the-strongdockerignorestrong-file&quot;&gt;The &lt;strong&gt;.dockerignore&lt;/strong&gt; file&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;.dockerignore&lt;/code&gt; file is the tool, that can help you to define the Docker &lt;strong&gt;build context&lt;/strong&gt; you really need. Using this file, you can specify &lt;strong&gt;ignore rules&lt;/strong&gt; and &lt;strong&gt;exceptions&lt;/strong&gt; from these rules for files and folder, that won&apos;t be included in the &lt;strong&gt;build context&lt;/strong&gt; and thus won&apos;t be packed into an archive and uploaded to the Docker server.&lt;/p&gt;
&lt;h1 id=&quot;why-should-you-care&quot;&gt;Why should you care?&lt;/h1&gt;
&lt;p&gt;Indeed, why should you care? Computers today are fast, networks are also pretty fast (hopefully) and storage is cheap. So, this &amp;quot;tax&amp;quot; may be not that big, right?
I will try to convince you, that you should care.&lt;/p&gt;
&lt;h2 id=&quot;reason-1-docker-image-size&quot;&gt;Reason #1: Docker image size&lt;/h2&gt;
&lt;p&gt;The world of software development is shifting lately towards &lt;em&gt;continuous delivery&lt;/em&gt;, &lt;em&gt;elastic infrastructure&lt;/em&gt; and &lt;em&gt;microservice architecture&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;How is that related?&lt;/p&gt;
&lt;p&gt;Your systems are composed of multiple components (or &lt;em&gt;microservices&lt;/em&gt;), each one of them running inside Linux container. There might be tens or hundreds of services and even more service instances. These service instances can be built and deployed independently of each other and this can be done for &lt;strong&gt;every single code commit&lt;/strong&gt;. More than that, &lt;em&gt;elastic infrastructure&lt;/em&gt; means that new compute nodes can be added or removed from the system and its microservices can move from node to node, to support scale or availability requirements. That means, your Docker images will be frequently built and transferred.&lt;/p&gt;
&lt;p&gt;When you practice continuous delivery and microservice architecture, image size and image build time &lt;strong&gt;do matter&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;reason-2-unintended-secrets-exposure&quot;&gt;Reason #2: Unintended secrets exposure&lt;/h2&gt;
&lt;p&gt;Not controlling your &lt;strong&gt;build context&lt;/strong&gt;, can also lead to an unintended exposure of your code, commit history, and secrets (keys and credentials).&lt;/p&gt;
&lt;p&gt;If you copy files into you Docker image with &lt;code&gt;ADD .&lt;/code&gt; or &lt;code&gt;COPY .&lt;/code&gt; command, you may unintendedly include your source files, whole &lt;code&gt;git&lt;/code&gt; history (a &lt;code&gt;.git&lt;/code&gt; folder), secret files (like &lt;code&gt;.aws&lt;/code&gt;, &lt;code&gt;.env&lt;/code&gt;, private keys), cache and other files not only into the Docker &lt;strong&gt;build context&lt;/strong&gt;, but also into the final Docker image.&lt;/p&gt;
&lt;p&gt;There are multiple Docker images currently available on DockerHub, that expose application source code, passwords, keys and credentials (for example &lt;a href=&quot;http://thehackernews.com/2016/07/vine-source-code.html&quot;&gt;Twitter Vine&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&quot;reason-3-the-docker-build---cache-invalidation&quot;&gt;Reason #3: The Docker build - cache invalidation&lt;/h2&gt;
&lt;p&gt;A common pattern is to inject an application&apos;s entire codebase into an image using an instruction like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COPY . /usr/src/app
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we&apos;re copying the &lt;strong&gt;entire&lt;/strong&gt; &lt;strong&gt;build context&lt;/strong&gt; into the image. It&apos;s also important to understand, that every Dockerfile command generates a new layer. So, if any of included file changes in the entire build context, this change will invalidate the build cache for &lt;code&gt;COPY . /opt/myapp&lt;/code&gt; layer and a new image layer will be generated on the next build.&lt;/p&gt;
&lt;p&gt;If your working directory contains files that are frequently updated (logs, test results, git history, temporary cache files and similar), you are going to regenerate this layer for every &lt;code&gt;docker build&lt;/code&gt; run.&lt;/p&gt;
&lt;h1 id=&quot;the-codedockerignorecode-syntax&quot;&gt;The &lt;code&gt;.dockerignore&lt;/code&gt; syntax&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;.dockerignore&lt;/code&gt; file is similar to &lt;code&gt;gitignore&lt;/code&gt; file, used by &lt;code&gt;git&lt;/code&gt; tool. similarly to &lt;code&gt;.gitignore&lt;/code&gt; file, it allows you to specify a pattern for files and folders that should be ignored by the Docker client when generating a &lt;strong&gt;build context&lt;/strong&gt;. While &lt;code&gt;.dockerignore&lt;/code&gt; file syntax used to describe &lt;strong&gt;ignore patterns&lt;/strong&gt; is similar to &lt;code&gt;.gitignore&lt;/code&gt; it&apos;s not the same.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.dockerignore&lt;/code&gt; pattern matching syntax is based on Go &lt;code&gt;filepath.Match()&lt;/code&gt; function and includes some additions.&lt;/p&gt;
&lt;p&gt;Here is the complete syntax for the &lt;code&gt;.dockerignore&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pattern:
    { term }
term:
    &apos;*&apos;         matches any sequence of non-Separator characters
    &apos;?&apos;         matches any single non-Separator character
    &apos;[&apos; [ &apos;^&apos; ] { character-range } &apos;]&apos;
                character class (must be non-empty)
    c           matches character c (c != &apos;*&apos;, &apos;?&apos;, &apos;\\&apos;, &apos;[&apos;)
    &apos;\\&apos; c      matches character c

character-range:
    c           matches character c (c != &apos;\\&apos;, &apos;-&apos;, &apos;]&apos;)
    &apos;\\&apos; c      matches character c
    lo &apos;-&apos; hi   matches character c for lo &amp;lt;= c &amp;lt;= hi

additions:
  &apos;**&apos;        matches any number of directories (including zero)
  &apos;!&apos;         lines starting with ! (exclamation mark) can be used to make exceptions to exclusions
    &apos;#&apos;         lines starting with this character are ignored: use it for comments
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using the &lt;code&gt;!&lt;/code&gt; character is pretty tricky. The combination of it and patterns before and after line with the &lt;code&gt;!&lt;/code&gt; character can be used to create more advanced rules.&lt;/p&gt;
&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;# ignore .git and .cache folders
.git
.cache
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# ignore all *.class files in all folders, including build root
**/*.class
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# ignore all markdown files (md) beside all README*.md other than README-secret.md
*.md
!README*.md
README-secret.md
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&quot;next&quot;&gt;Next&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;RTFM - &lt;a href=&quot;https://docs.docker.com/engine/reference/builder/#/dockerignore-file&quot;&gt;https://docs.docker.com/engine/reference/builder/#/dockerignore-file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;.dockerignore&lt;/code&gt; in every project, where you are building Docker images&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Hope you find this post useful. I look forward to your comments and any questions you have.&lt;/p&gt;</content><author><name>Alexei Lednev</name><email>alexei.led@gmail.com</email></author><category term="Development" /><category term="Docker" /><category term="build" /><category term="dockerignore" /><category term="devops" /><summary type="html">TL;DR</summary></entry></feed>